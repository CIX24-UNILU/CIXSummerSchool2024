{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Rich Interactions in Mixed Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://jjdudley.com\">John Dudley</a>, Department of Engineering, University of Cambridge, UK.\n",
    "\n",
    "Note that portions of the <em>Statistical Decoding for Text Entry</em> portion of this notebook have been adapted from a tutorial originally created by <a href=\"http://pokristensson.com\">Per Ola Kristensson</a> (see material from 2019 Computational Interaction Summer School <a href=\"https://github.com/yujko/5thSummerSchoolCourseMaterials/tree/master/Day2-Jacques\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore <em>statistical decoding</em> as a technique for supporting rich interactions in Mixed Reality.\n",
    "\n",
    "\n",
    "| Outline                                               | Exercises |\n",
    "|:------------------------------------------------------|:-|\n",
    "| Motivation                                            |  |\n",
    "| Introduction to Statistical Decoding                  |  |\n",
    "| Sequence Decoding for Text Entry                      |  |\n",
    "| Statistical Decoding using Token Passing              | Group Discussion, Ex1. Test your understanding |\n",
    "| Decoding with substitutions, deletions and insertions | Group Discussion, Ex2. Use log probabilties |\n",
    "| Sequence Decoding for Hand Gesture Recogntion         |  |\n",
    "| Decoding with Simple Thresholds on Finger Bend Angles | Group Discussion, Ex3. Test your understanding |\n",
    "| Statistical Decoding using Finger Bend Angles         | Group Discussion, Ex4. Probabilistic recognition |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intelligent text entry method is a text entry method that _infers_ users' intended text. We have known how to design such methods for quite some time. One of the more interesting examples is a 12th century shorthand system called _Nova ars notaria_ (\"the new note art\") by the monk of John of Tilbury. What is interesting with this system is that the _design principles_ are known:\n",
    "\n",
    "* Simplify letters to line marks.\n",
    "* Compress common word stems into sequences of simple line marks and dots.\n",
    "* Identify common word stems by frequency analysis\n",
    "\n",
    "In other words, two fundamental principles underpin the design of any efficient text entry method:\n",
    "\n",
    "* Minimise users' effort in articulating their input.\n",
    "* Exploit the redundancies in natural languages using statistical language modelling.\n",
    "\n",
    "However, _how_ can we both minimise users' articulation effort and leverage the redundancies in natural languages? The solution is to perform _statistical decoding_. A statistical decoder is a generative probabilistic model that is capable of searching a vast hypothesis space in order to identify users' intended text given noisy observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are adressing is a user attempting to communciate information over some form of channel. In Human-Computer Interaction (HCI) we typically model a user transmitting _information_ to a computer system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term _information_ in HCI usually refers to characters (for example, typing on a keyboard), words (using speech recognition), or commands (for instance by using keyboard shortcuts or touchscreen gestures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, we intend to transmit a message $y$ via some form of signal $x$. In a perfect world this would be trivially achieved via a lookup-table. Unfortunately we live in an imperfect world and as a consequence our signal will be perturbed by noise in our neuromuscular system, device sensor imprecision, cognitive errors by the user, etc. Due to this inherent uncertainty it makes sense to model the problem _probabilistically_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wish to compute the probability of the message $y$ _given_ the signal $x$. This can be written mathematically as $P(y | x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The probability $P(y|x)$ is known as a _conditional probability_ and it is (by either the definition of conditional probability or as an axiom of probability):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y|x) = \\frac{P(x \\cap y)}{P(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We are going to assume $P(x)\\neq0$ and $P(y)\\neq 0$).\n",
    "\n",
    "The above equation states that the conditional probability of $y$ given $x$ is identical to the ratio of the _joint probability_ of $x$ and $y$ and the probability of $x$. The joint probability of $x$ and $y$ can also be written as $P(x,y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the conditional probability $P(y|x)$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(y|x) &= \\frac{P(x \\cap y)}{P(x)}\\\\\n",
    "P(x|y) &= \\frac{P(y \\cap x)}{P(y)} = \\frac{P(x \\cap y)}{P(y)}\\\\\n",
    "\\Rightarrow P(x \\cap y) &= P(x|y)P(y) = P(y|x)P(x)\\\\\n",
    "\\Rightarrow P(y|x) &= \\frac{P(x|y)P(y)}{P(x)}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last expression is known as _Bayes' rule_ (or theorem). Usually we have many possible messages that we wish to decode and $P(y|x)$ will then become the _posterior_ probability distribution, assigning a probability to every possible message. Our objective is to compute this posterior probability distribution and select the most probable message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we are usually only interested in the most probable message $\\hat{y}$ we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y}=\\underset{y}{\\arg\\max}\\left[P(y|x)\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen that the conditional probability $P(y|x)$ can be written using Bayes' rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y}=\\underset{y}{\\arg\\max}\\left[\\frac{P(x|y)P(y)}{P(x)}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we are only interested in the message that maximises the conditional probability of the message given the signal, the denominator $P(x)$ will be invariant and can therefore be dropped:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{y}=\\underset{y}{\\arg\\max}\\left[P(x|y)P(y)\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(x|y)$ is the likelihood of the signal $x$ given a particular hypothesis for what the message $y$ could be. $P(y)$ is the _prior_ probability of the message, that is, without taking any signal into account. For instance, if a system can only recognise two messages, $x_1$ and $x_2$, and both are equally likely in the absence of any additional information, then the prior probability of either $x_1$ or $x_2$ is $0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the highest probable message $y$ is a _search problem_. We search by consulting a model of the likelihood of a signal $x$ given a message $y$ under consideration and by consulting a model of the prior probability of a message $y$ without any consideration to any signal. This search will generate _hypotheses_ and these hypotheses will have probabilities assigned to them. Usually, the hypothesis with the highest probability assigned to it is our preferred hypothesis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{\\text{hypothesis}}=\\underset{\\text{hypotheses}}{\\arg\\max}\\left(\\text{likelihood model}\\cdot\\text{prior model}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Decoding for Text Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now switch to the [Sequence Decoding for Text Entry notebook](TextEntry.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Decoding for Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now switch to the [Sequence Decoding for Hand Gesture Recognition notebook](HandGestures.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
