{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7182e8c9-148f-4b1b-a273-8825a0709bce",
   "metadata": {},
   "source": [
    "# Design, parametrization and Selection of Behavioral model\n",
    "\n",
    "### Teacher\n",
    "Gilles Bailly\n",
    "\n",
    "### Requirements\n",
    "- HCI basics\n",
    "- python programming\n",
    "- Mathematics basics (probability, log, exp, min, max etc.)\n",
    "- Reinforcement learning (very basics)\n",
    "\n",
    "### Libraries\n",
    "- tested with python: 3.7,\n",
    "- numpy: 1.25.0, pandas: 1.4.2, matplotlib: 3.5.3, seaborn: 0.11.2, scipy: 1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2a4ba-4842-4233-9b4e-f4234269d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import random\n",
    "from scipy.optimize import brute, differential_evolution\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('Python version', sys.version)\n",
    "print('numpy:', np.__version__, ', pandas:', pd.__version__, ', matplotlib:', matplotlib.__version__, ', seaborn:', sns.__version__, ', scipy:', scipy.__version__)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bcfa7-ef5a-41cb-ad4f-df195291c258",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The goal of this exercise is to provide guidelines when building and testing models to predict and explain human behavior.\n",
    "\n",
    "### Acquired skills at the end of the exercise\n",
    "- Use simple reinforcement learning models often used in decision making and neurosciences\n",
    "- Develop models for sequential decision making\n",
    "- Simulate models\n",
    "- Adjust the parameters of a model\n",
    "- Compare models with the tools log-likelihood and BIC score\n",
    "- Use parameter recovery and model recovery\n",
    "\n",
    "### Organization\n",
    "This exercise is divided into five parts:\n",
    "- Part 1: Familiarization with the data (10mn)\n",
    "- Part 2: My first very simple model (5mn)\n",
    "- Part 3: Parameter fits (60mn)\n",
    "- Part 4: Model comparison and selection (30mn)\n",
    "- Part 5: Model simulation (30mn)\n",
    "- Part 6: Model evaluation - step-by-step (30mn)\n",
    "- model evaluation\n",
    "\n",
    "### Scientific articles\n",
    "- Wilson, R. C., & Collins, A. G. (2019).  ",
    "Ten simple rules for the computational modeling of behavioral data.  ",
    "Elife, 8, e49547\n",
    "- Tovi Grossman, Pierre Dragicevic, and Ravin Balakrishnan. 2007. Strategies for accelerating on-line learning of hotkeys. ACM CHI '07. 1591–1600. https://doi.org/10.1145/1240624.1240865\n",
    "- Gilles Bailly, Mehdi Khamassi, and Benoît Girard. 2023. Computational Model of the Transition from Novice to Expert Interaction Techniques. ACM Trans. Comput.-Hum. Interact. 30, 5, Article 66 (October 2023), 33 pages. https://doi.org/10.1145/3505557\n",
    "-\tGilles Bailly, Eric Lecolinet, and Laurence Nigay. 2016. Visual Menu Techniques.  ACM Comput. Surv. 49, 4, Article 60 (February 2017), 41 pages. https://doi.org/10.1145/30021715\n",
    "-\tAdrian E Raftery. 1995. Bayesian model selection in social research. Sociological methodology (1995), 111–163."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d272cdf-5317-4081-8ef8-ef2a01ab2e57",
   "metadata": {},
   "source": [
    "<img style=\"float: right;margin-left:20pt;width:25%\" src=\"images/learning_decision.pdf\">\n",
    "\n",
    "# PART 0: Context (1mn)\n",
    "\n",
    "### Questions?\n",
    "\n",
    "Do I use my experience or google Maps to choose my route?\n",
    "\n",
    "Do I answer my email on my smartphone or on my PC?\n",
    "\n",
    "Do I use the Da Vinci Surgical Robot to operate my patient or  ",
    "the traditional laparoscopic instruments?\n",
    "\n",
    "How do I reach this object with my robotic prosthesis?\n",
    "\n",
    "Do I use Menu or keyboard shortcut?\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<strong>Question:</strong> What do these questions have in common?\n",
    "</div>\n",
    "\n",
    "\n",
    "### HCI and Interaction model\n",
    "HCI is about: \n",
    "- understanding human behavior with interactive systems\n",
    "- designing new forms of interaction.\n",
    "\n",
    "Interaction models are essential for\n",
    "- understanding and predicting user behavior and\n",
    "- thus predicting the effectiveness of an interface.\n",
    "- optimizing interfaces during the design phase or\n",
    "- dynamically adapt the interfaces during the interaction.\n",
    "\n",
    "# PART 1 : FAMILIARIZATION WITH THE DATA (10mn)\n",
    "\n",
    "\n",
    "\n",
    "### Expert interaction technniques, eg. keyboard shortcuts\n",
    "\n",
    "<div  style=\"float:right;margin-left:20pt; width:450px\"> \n",
    "    <img style=\"width:100%\" src=\"images/audio_menu.png\"><br/>\n",
    "    <strong>Figure 1:</strong> Interaction technique playing the kebyoard shortcut when a command is executed from the menu [Grossman et al. 2007]\n",
    "    </div>\n",
    "\n",
    "How to motivate users to adopt efficient expert interaction techniques to accomplish a task? \n",
    "\n",
    "Typically, keyboard shortcuts, e.g. Ctrl+S for the Save command, are particularly effective. Unfortunately, they are seldom used: many users, even experts, continue to use menus rather than making the transition to keyboard shortcuts.\n",
    "Many interaction techniques are regularly proposed to promote the use of keyboard shortcuts [Bailly et al. 2016]. \n",
    "\n",
    "For example, one interaction technique (audio feedback) plays the name of the keyboard shortcut when the command is selected in the menu (Figure 1) [Grossman et al. 2007].\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "<div  style=\"float:right;margin-left:20pt; width:450px\"> \n",
    "<img src=\"images/task.png\" width=\"400\"/><br/>\n",
    "<strong>Figure 2:</strong> A trial. The stimulus is displayed at the bottom of the screen. The participant execute the command either with the menu or, if she knows it, with its corresponding keyboard shortcut [Grossman et al. 2007]    \n",
    "</div>\n",
    "\n",
    "In [Grossman et al. 2007], the authors conducted an experimental study to compare several interaction techniques to promote the use of keyboard shortcuts (Figure 2). The authors compare three techniques (Traditional, Audio, Disable). Each participant tests only one technique and execute 720 commands (each command has different frequencies. Some commands appear frequently, others rarely). In this exercise, we only consider the Audio technique (Figure 1). \n",
    "\n",
    "At each attempt, the target command appears at the bottom of the screen and the user executes it either with the MENU method or with the SHORTCUT method (Figure 2). But, from a cognitive point of view, we consider that the user chooses one of these three strategies:\n",
    "\n",
    "### Srategies\n",
    "\n",
    "- MENU: I select the command with the MENU method.\n",
    "- SHORTCUT: I select the command with the Keyboard shortcuts method\n",
    "- LEARNING: I learn the keyboard shortcut but execute the command in the menu (it takes more time)\n",
    "\n",
    "At each trial, the log contains the strategy (menu, shortcut, learning), the time and whether the command was executed correctly or not.\n",
    "\n",
    "\n",
    "### Make sense of the data \n",
    "\n",
    "Let's have quick look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1947610-2c7b-4467-9800-adfc1acc461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv( './data/audio_hotkey.csv' )\n",
    "print( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1524301-94d7-4a78-af5e-26c19aadfeac",
   "metadata": {},
   "source": [
    "The columns corresponding to the experimental design:\n",
    "- participant: id of the participant\n",
    "- technique_name: the name of the technique (in this exercise, we only consider audio)\n",
    "- block_id: id of the block\n",
    "- trial_id: id of the trial\n",
    "- cmd_input: the id of the command to execute\n",
    "- cmd_name: the name of the command to execute\n",
    "- cmd_frequency : the number of occurecnces of this command\n",
    "\n",
    "Logged data for each participant\n",
    "- time: The time to execute the command (seconds)\n",
    "- success: 1 if the participant successfully executed the command, 0 otherwise\n",
    "- strategy: The most important column in this exerice. This column indicates the chosen strategy: Menu (0); Shortcut (1), Learning (2)\n",
    "\n",
    "The following code provides the unique values for the most important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d7ab5-6485-4164-8a0f-84f35880a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary\n",
    "print( \"Participant ids ( total=\", len( data['participant'].unique() ), \"): \", data['participant'].unique() )\n",
    "print( \"Technique :\"        , data['technique_name'].unique()[0] )\n",
    "print( \"Number of blocks:\"  , data[ 'block_id' ].max()+1 )\n",
    "print( \"Number of trials:\"  , data[ 'trial_id' ].max() + 1 )\n",
    "print( \"Command ids: \"      , data[ 'cmd_input' ].unique() )\n",
    "print( \"Command names:\"     , data[ 'cmd_name' ].unique() )\n",
    "print( \"Command occurences:\", np.sort( data[ 'cmd_frequency' ].unique() ) )\n",
    "print( \"Strategy values:\"   , np.sort( data[ 'strategy' ].unique() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07546b-f1c8-4784-8753-6ed101f39b2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Analysis at the poplulaion level\n",
    "The most common analysis in this type of articles is to compare the evolution of shortcut use per block (each block contains 60 trials). We expect the participants to learn and use more shortcuts with practice. Is it the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081a1aa-01d6-42bc-9d55-f82cfbfd1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data [ 'shortcut use' ] = 0 #percentage\n",
    "data.loc[ ( data[ 'success' ] == 1 ) & ( data[ 'strategy' ] == 1 ), 'shortcut use' ] = 100\n",
    "data_view = data.groupby( [ \"block_id\" , \"participant\"] ).mean('shortcut use' )\n",
    "data_view = data_view.reset_index()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8.0, 4.0]\n",
    "rel = sns.lineplot( data = data_view, x = 'block_id', y = 'shortcut use', linewidth = 5, color = \"green\" )\n",
    "info = rel.set( title= 'Shortcut use (%) per block', xlabel = \"Block id\", ylabel= \"Shortcut use (%)\", ylim = [0,100], xlim = [0,11] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fbaf9-e3aa-4c05-8ad8-27329b2d519d",
   "metadata": {},
   "source": [
    "As expected, at the **population** level (the data of the 14 participants are aggregaed), shortcut use increases wih practice. However, the large confidence inerval suggests different user behaviors. Let analyze the data at the **individual** level\n",
    "\n",
    "#### Analysis at the individual level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76587b-9c24-4ff9-9a4e-0776ac0b67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [8.0, 4.0]\n",
    "rel = sns.lineplot( data = data_view, x = 'block_id', y = 'shortcut use', hue = 'participant' )\n",
    "f = rel.set( title= 'Shortcut use (%) per block and per participant', xlabel = \"Block id\", ylabel= \"Shortcu use (%)\", ylim = [0,100], xlim = [0,11] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587bc11-2f64-4a7d-a293-c4e7b9b0e27b",
   "metadata": {},
   "source": [
    "The 14 simple lines indicate the behavior of each participant. We observe quite different behvaiors. For instance P4 successfully executed shortcuts more than 85% of the time while P10 never successfully executed shortcuts. <strong>This is one key difficulty when elaborating models of decision making: users can have radically different strategies and aggregating data does not always make sense.<strong>\n",
    "\n",
    "We can also visualize the data of a single participant to analyse how the different strategies (choices) evolve with practice (trial id) for each command.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> Analyze the data of different participants, in particular, P5, P4 and P10\n",
    "</div>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66885180-c9e8-436c-a56f-e7efbf4c976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a participant\n",
    "df = data[ data[ 'participant' ] == 4 ]\n",
    "\n",
    "# Color strategies: NOVICE 0: blue, EXPERT 1: green, LEARNING 2: red\n",
    "strategy_palette = { 0: (0, 0, 1), 1: (0, 1, 0), 2: (1, 0, 0) }\n",
    "\n",
    "plt.figure( figsize=(16, 4) )\n",
    "\n",
    "for cmd in df[ 'cmd_input' ].unique() :    \n",
    "    plt.axhline( cmd, linewidth=1, color='lightgrey' )\n",
    "    g = sns.scatterplot( x = 'trial_id', y = 'cmd_input', hue=\"strategy\", palette = strategy_palette, data = df )\n",
    "g.set( title= \"Participant strategy (Menu, Learning, Expert) per trial\", xlabel = \"Trial id\", ylabel= \"Command id\", yticks = np.arange(start=0, stop=14, step=1) )\n",
    "\n",
    "legend_lines = [Line2D( [0], [0], marker='o', color = \"white\", markerfacecolor=strategy_palette[ 0 ], lw=3),\n",
    "             Line2D( [0], [0], marker='o', color = \"white\", markerfacecolor = strategy_palette[ 2 ], lw=3),\n",
    "             Line2D( [0], [0], marker='o', color = \"white\", markerfacecolor = strategy_palette[ 1 ], lw=3)]\n",
    "l = plt.legend(legend_lines, ['Menu', 'Learning', 'Expert' ], fontsize='xx-small', ncol = 1, loc = 'upper left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8b7c8-04fe-4eb8-a922-f158c79eeb45",
   "metadata": {},
   "source": [
    "We observe that:\n",
    "- some commands are more frequent\n",
    "- participants tend to choose MENU (blue) first, then LEARNING (red) and finally SHORTCUT (green).\n",
    "- The evolution of the strategies depends on the command frequency and the participant id.\n",
    "\n",
    "### More information\n",
    "You can find more information in these articles [Grossman et al. 2007], [Bailly et al. 2017].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa24e6d-f186-4e14-bb97-eda03f2ed3ab",
   "metadata": {},
   "source": [
    "# PART 2: MY FIRST VERY BASIC MODEL (5mn)\n",
    "<strong>Outline:</strong> \n",
    "<ol>\n",
    "    <li> Problem formulation and approach</li>\n",
    "    <li> First (very basic) model </li>\n",
    "</ol>\n",
    "\n",
    "<div  style=\"float:right;margin-left:20pt; width:450px\"> \n",
    "    <img src=\"images/RL.png\" width=\"100%\"/>\n",
    "    <strong>Figure 3</strong>: Reinforcement Learning (RL) Framework.\n",
    "</div>\n",
    "\n",
    "### 2.1 Problem formulation and approach\n",
    "We formulate the problem of the transition from MENU to SHORTCUT as a <strong>Reinforcement Learning </strong> (RL) problem.\n",
    "\n",
    "The interface is the environment and the user is the agent. \n",
    "\n",
    "Given a command s to execute (<strong>State</strong>), the agent chooses an <strong>Action</strong> (or strategy) among MENU, SHORTCUT and LEARNING. \n",
    "\n",
    "The <strong>Reward</strong> (here cost) r, is the time it takes the agent to execute the command (to simplify we make the assumption that the user does not make errors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd31ce-eaf0-4309-8a18-6a750f4fc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State == command id (int)\n",
    "\n",
    "# Action\n",
    "class Action(object):\n",
    "    MENU = 0\n",
    "    SHORTCUT = 1\n",
    "    LEARNING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476a391-bf26-48a4-a562-c3aaaeae4734",
   "metadata": {},
   "source": [
    "### 2.2 First (very basic) model\n",
    "<strong> Biased Random model ($M_R$). </strong> This model chooses a random strategy with a bias for one of the options (here SHORTCUT). In our case, the bias is captured with the parameter $\\theta$ which is between 0 and 1 such that the probability of using the strategy (action) a for the command c:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$  P(c, a^k) = \\theta \\quad  if \\quad  k==SHORTCUT $$ \n",
    "$$ \n",
    "P(c, a^k) = \\frac{1-  \\theta }{2} \\quad if \\quad k == MENU \\quad or \\quad LEARNING $$\n",
    "</div>\n",
    "<strong> This model has a single parameter $ \\theta $. </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce088e-bc4a-450a-9202-9d6ed5851519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing the list of parameters (here only one, 'theta') and a default value (here 0.2) \n",
    "rand_parameter = { 'THETA': 0.2 }\n",
    "\n",
    "###################################################\n",
    "# parameter : dict< name, value>\n",
    "# c         : command\n",
    "# OUTPUT: res (np.array) len ( res ) == 3; np.sum( res ) = 1\n",
    "#########################################\n",
    "def rand_action_probability( parameter, c ) :\n",
    "    theta = parameter[ 'THETA' ]\n",
    "    return np.array( [(1 - theta) / 2, theta, (1 - theta) / 2 ] )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cdf58-a270-41d6-89f5-4a702c674d13",
   "metadata": {},
   "source": [
    "This model is clearly not realistic. For instance, It does not take into account the history of executed commands / chosen strategies. However, it will help us to understand the basics of modeling. \n",
    "\n",
    "<strong> How to choose the value of the parameter $\\theta$? </strong>\n",
    "\n",
    "<strong>Answer: Parameter fits!</strong>\n",
    "\n",
    "# PART 3: Parameter fits (60mn)\n",
    "\n",
    "<strong>The objective of this part is to identify the parameter values that best describe a sequence of observations </strong>\n",
    "\n",
    "<ol>\n",
    "<li> Sequence of observations </li>\n",
    "<li> Likelihood </li>\n",
    "<li> Log-Likelihood </li>\n",
    "<li> Maximum Log-Likelihood </li>\n",
    "<li> Computational solving </li>\n",
    "<li> Conclusion </li>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "#### 3.1 Sequence of observations $X$\n",
    "In the Grossman et al. experiment, a participant $p$ performs 720 trials. At each trial $t$, the participant chooses an action (strategy) producing an observation $x_{t}^p \\in \\{MENU, LEARNING, SHORTCUT\\}$. The sequence of observations of the participant $p$ is thus: $X = X_{1:720}^p ={x_1^p,..., x_{720}^p}$.\n",
    "\n",
    "<center><img src=\"images/sequence_observations.pdf\" width=\"600\"/></center> \n",
    "\n",
    "<center><strong>Figure 4:</strong> Example of a possible sequence of observations from participant 5 (considering only 8 trials for clarity)</center> \n",
    "\n",
    "#### 3.2 Likelihood $ \\mathcal{L}$\n",
    "The likelihood $ \\mathcal{L}$ is a function computing how well a model (with its parameters) describes a sequence of observations. More precisely, $ \\mathcal{L}(M_R, \\theta, X) $ computes the probability of the model ($M_R$) with its parameters ($\\theta$) to generate the <strong> whole sequence of observations </strong> ($X$) of a given participant. \n",
    "\n",
    "The probability of the model to generate all observations is the product of the probabilities to generate <strong>each observation</strong> :\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{L}(M_R, \\theta, X) = P(M_R, \\theta|X) = P(M_R, \\theta | {x_1,..,x_{720}}) = \\prod_{i=1:720} P(x=x_i | M_R, \\theta ) )$$\n",
    "\n",
    "(we simplified the Bayes theorem for this exercice)\n",
    "\n",
    "<center><img src=\"images/likelihood.pdf\" width=\"620\"/></center> \n",
    "\n",
    "<center><strong>Figure 5:</strong> Estimation of the likelihood (0.00016384), considering $M_R$, $\\theta=0.2$ </center> <p/>\n",
    "\n",
    "Notice that $\\mathcal{L}(\\theta, M_R | X) \\in [0,1]$. The closer to 1, the better the model explains the data.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> Implement the function likelihood. At this stage, ignore the argument T. Instead of returning only the likelihood, return both the likelihood and the vector of probabilities.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a8650-e730-46e3-b4af-d2be0bb38df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# m: Model. It is a function with two arguments : \n",
    "#        - parameter its set of parameters\n",
    "#        - an observation (in our case the command c)\n",
    "#        the function returns the probabilities of the model to generate each strategy\n",
    "# parameter: the set of parameters of the model m, \n",
    "# X: Sequence of observations (strategies)\n",
    "# C: Sequence of commands\n",
    "# T: Sequence of time (duration to execute the command with the chosen strategy)\n",
    "# OUTPUT : [ likelihood, vector of probabilities ]\n",
    "# At this stage, we ignore T\n",
    "#################################\n",
    "def likelihood( m, parameter, X, C, T) :\n",
    "    prob_vec = np.ones( len(X) )\n",
    "\n",
    "    #####################\n",
    "    # TODO\n",
    "    # l = ...        \n",
    "    ######################\n",
    "    return [ l, prob_vec ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dc1b2-4715-4a7e-b480-00ef12f64cf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> We can now estimate the likelihood of the Biased Random model to generate the 20 first observations for different participants and different values of theta. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22c4ef-8744-44fc-96bc-907e87ee52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p     = 5     # Participant 5\n",
    "t_max = 20    # the first 20 trials\n",
    "rand_parameter[ 'THETA' ] = 0.2\n",
    "\n",
    "# select a subset of data\n",
    "data = pd.read_csv( './data/audio_hotkey.csv' )\n",
    "X     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy() # list of observed strategies\n",
    "C     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy() # list of commands\n",
    "T     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy() # list of observed time\n",
    "\n",
    "#estimate likelihood\n",
    "likelihood( rand_action_probability, rand_parameter, X, C, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81007792-1a3f-4c00-bc70-c77ce6125102",
   "metadata": {},
   "source": [
    "Why this likelihood is important, useful and appropriate? \n",
    "\n",
    "Because we really compute the probability to generate the <strong> whole </strong> sequence of observations. Two errors that we sometimes see: \n",
    "- estimating the probability of generating only the <strong>last</strong> observation, ignoring the capacity of the model to generate the N-1 observations. In our context, estimating the last observation is quite easy (It is likely to be SHORTCUT). What is complex / important is to predict all intermediate observations, in particular when users will choose LEARNING.\n",
    "- estimating aggregate distributions. For instance, both the model and the participant produces 50% MENU, 25% LEARNING and 25% SHORTCUT. It is not sufficient because it does not consider the <strong> order </strong> of the sequence of observations. \n",
    "\n",
    "<center><img src=\"images/proportion.pdf\" width=\"580\"/></center> \n",
    "\n",
    "<center><strong>Figure 6:</strong> Two sequences of observations with the same distribution of strategies (50% MENU, 25% LEARNING and 25% SHORTCUT), but very different user behaviors.</center> \n",
    "<p/>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Now, estimate the likelihood of the 720 observations (t_max = 720) instead of 20. Do you see a problem?\n",
    "</div>\n",
    "Let see if you have the same problem with log-likelihood...\n",
    "\n",
    "#### 3.3 Log-Likelihood $log\\mathcal{L}$\n",
    "We introduce the log-likelihood  $log\\mathcal{L}(\\theta | X)$\n",
    "$$log\\mathcal{L}(\\theta | X) = log( \\mathcal{L}(\\theta | X) ) $$\n",
    "$$ log\\mathcal{L}(\\theta | X) = log( \\prod_{i=1:720} P(x=x_i | \\theta ) ) )  $$\n",
    "$$ log\\mathcal{L}(\\theta | X) = \\sum_{1:720} log P(x=x_i | \\theta)  $$\n",
    "\n",
    "$log\\mathcal{L}$ is negative because $P \\in [0,1]$. So, the closer to zero, better the model explains the data. \n",
    "The key property of the $log\\mathcal{L}$ is that it relies on a sum ($\\sum $) instead of a product ($\\prod$).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> implement the log_likelihood function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e8325-af03-4de3-b5db-c910a1ac1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# m:      Model func, (m has two arguments: it set of parameters and a command c)\n",
    "# param:  Parameters of the model m, \n",
    "# X:      sequence of strategies (observations)\n",
    "# C:      Sequence of commands\n",
    "# T:      Sequence of time (duration to execute the command with the chosen strategy (observations)\n",
    "# OUTPUT: [ log-likelihood, vector of probabilities ]\n",
    "#################################\n",
    "def log_likelihood( m, parameter, X, C, T):\n",
    "    prob_vec = np.ones( len(X) )\n",
    "    for i in range(0, len(X) ) :\n",
    "        prob_vec[i] = m( parameter, C[i] )[ X[i] ]\n",
    "    #################\n",
    "    # TODO\n",
    "    # ll = \n",
    "    #################\n",
    "    return ll, prob_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358dbb09-6167-460a-8f4b-fdc955ab694b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Estimate the <strong>log-likelihood</strong> of the model to generate the 720 first observations of the P5 (and P6) for different values of theta. Do you still see a problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18c89d-11af-4718-85cb-da157efc8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the 20 first observations (strategy) of the participant 5\n",
    "p     = 5    # Participant 5\n",
    "t_max = 720   # all trials\n",
    "C     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy()\n",
    "X     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy()\n",
    "T     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy()\n",
    "\n",
    "rand_parameter[ 'THETA' ] = 0.2\n",
    "log_likelihood( rand_action_probability, rand_parameter, X, C, T )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed517b90-f407-4f7e-883c-0d374b1dc9bb",
   "metadata": {},
   "source": [
    "Great! we now have a robust metric to estimate how well a model (with a given set of parameter) explains a sequence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06c7da-9eff-4b20-829a-7066f34ba424",
   "metadata": {},
   "source": [
    "<img style=\"float: right;margin-left:20pt;width:30%\" src=\"images/optimisation.pdf\">\n",
    "\n",
    "#### 3.4 Maximum log-likelihood\n",
    "\n",
    "Remember our goal: Find the value of the parameter $\\theta$ that best explains the sequence of observations. Technically, it means to find  $\\theta$ vaule that :\n",
    "- maximizes the likelihood: $argmax_{\\theta}( \\mathcal{L}(\\theta | X) )$\n",
    "- maximizes the log-likelihood: $argmax_{\\theta}( log\\mathcal{L}(\\theta | X) )$\n",
    "\n",
    "In practice, <strong> how to explore the parameter space? and find $\\theta$ that maximizes the log-likelihood? </strong>\n",
    "We can try to do it manually (3.5) when the problem is simple, otherwise it requires computational solving (3.6).\n",
    "\n",
    "#### 3.5 Manual solving\n",
    "When the number of parameters is small (1,2, 3), it might be sufficient to <strong>manually</strong> explore the parameter space. In our context, we have a simple parameter $\\theta$. we can thus easily find a $\\theta$ maximizing the log-likelihood. It has two additional advantages:\n",
    "- We can compare the results obtained manually with those obtained with an optimization method.\n",
    "- We can visually explore the parameter space (plot log-likelihodd as a function of $\\theta$)\n",
    "and see if there are some local optima.\n",
    "\n",
    "To **manually** find the best $\\theta$ value (best_theta) and max $log\\mathcal{L}$ (max_ll), we simply use a simple ``for'' loop as illustrated below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf0ab8-bbef-4ccd-86aa-f2669f4eae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the data\n",
    "p     = 5      # Participant 5\n",
    "t_max = 720    # the first 20 trials\n",
    "C     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy()\n",
    "X     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy()\n",
    "T     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy()\n",
    "\n",
    "EPS = 0.01   #used to avoid side effects\n",
    "\n",
    "# Initial values\n",
    "theta_vec  = np.linspace( EPS, 1-EPS, 20 ) # all theta values\n",
    "ll_vec     = []   # vector containing the log_likelihood for all theta values\n",
    "max_ll     = -1000000000\n",
    "best_theta = 0\n",
    "\n",
    "t = time.process_time()\n",
    "\n",
    "################\n",
    "# key part\n",
    "for theta in theta_vec:\n",
    "    rand_parameter[ 'THETA' ] = theta\n",
    "    ll, prob_vec = log_likelihood( rand_action_probability, rand_parameter, X, C, T )\n",
    "    ll_vec.append( ll )\n",
    "    if ll > max_ll :\n",
    "        max_ll = ll\n",
    "        best_theta = theta\n",
    "#################\n",
    "\n",
    "manual_elapsed_time = round( time.process_time() - t, 3 )\n",
    "manual_theta        = round( best_theta, 3)\n",
    "manual_ll           = round( max_ll, 3)\n",
    "\n",
    "print( \"Method: Manual \\t log_likelihood:\", manual_ll, \"\\t theta:\", manual_theta, \"\\t elasped time:\", manual_elapsed_time ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf51eb9-072f-49b5-8afa-4f98ba11a931",
   "metadata": {},
   "source": [
    "We can also plot the log-likelihood as a function of theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ed0f5-a186-474e-982a-9d30fc6a521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [14.00, 4.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# plot the global view\n",
    "f0 = plt.figure( figsize=(16, 2) )\n",
    "f, axes = plt.subplots(1, 2)\n",
    "axes[0].plot( theta_vec, ll_vec )\n",
    "#axes[0].text( best_theta - 0.2, max_ll - 400, label ) \n",
    "axes[0].axhline( y = max_ll,     xmin=0, xmax=1, linestyle = \"--\" )\n",
    "axes[0].axvline( x = best_theta, ymin=0, ymax=1, linestyle = \"--\" )\n",
    "axes[0].set( xlabel= \"Theta value\", ylabel= \"Log likelihood\", title = \"Whole parameter space\" )\n",
    "\n",
    "# plot the local view centered around the max log likelihood\n",
    "delta_x = 0.1\n",
    "delta_y = 100\n",
    "x_min = best_theta - delta_x\n",
    "x_max = best_theta + delta_x\n",
    "y_min = max_ll - delta_y\n",
    "y_max = max_ll + delta_y\n",
    "axes[1].plot( theta_vec, ll_vec )\n",
    "#axes[1].text( best_theta + delta_x / 2, ll_min -delta_y /2, label ) \n",
    "axes[1].axhline( y = max_ll,     xmin = 0, xmax = 1, linestyle = \"--\" )\n",
    "axes[1].axvline( x = best_theta, ymin = 0, ymax = 1, linestyle = \"--\" )\n",
    "axes[1].axis( xmin = x_min, xmax = x_max, ymin = y_min, ymax = y_max )\n",
    "axes[1].set( xlabel= \"Theta value\", ylabel= \"Log likelihood\", title = \"View centered on the max log -likelihood\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bae7f8-3ff4-4a7c-ac2b-9af6c0502e6a",
   "metadata": {},
   "source": [
    "Great! we found the best $\\theta$ and max log-likelihood $log\\mathcal{L}$.\n",
    "\n",
    "#### 3.6 Computational solving \n",
    "Manually epxloring the design space might be challenging when the number of parameters is increasing. Several optimization functions are available in python, e.g. with the library scipy.optimize. They generally take as argument:\n",
    "- func: an objective function to be <strong>minimized</strong> (not maximized...)\n",
    "- bounds (or sometimes ranges):  (min, max) pairs defining the finite lower and upper bounds for the optimizing argument of func.\n",
    "\n",
    "They return (at least) \n",
    "- the solution array x, ie. the array of parameter values minimizing func\n",
    "- the minimal value of func: fun = func( x )\n",
    "\n",
    "Several optimization approaches are available:\n",
    "- grid search, but it becomes expensive in large parameter space\n",
    "- gradient descent, but it can fall into local optima\n",
    "- genetic algorithms (CMA-ES, NSGA-2), less prone to local optima.\n",
    "- etc.\n",
    "\n",
    "In this exercice, we will test <strong>brute</strong> (grid search) and <strong>differential evolution</strong> as optimisation method from scipy.optimize. We will check that \n",
    "- they return the same best_theta,\n",
    "- they return the same max log-likelihood\n",
    "- they are at least as fast as our naive implementation \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO 1</strong>: implement the method inv_log_likelihood, to be compatible with common optimization methods: (1) we want to minimize, not maximize; (2) the first argument should be an array of parameter values.\n",
    "\n",
    "<strong>TODO 2</strong>: compare the grid search algorihm (brute) and the differential evolution algorithm (differential_evolution) with our default implementation.\n",
    "\n",
    "<strong>TODO 3</strong>: Test smaller and larger values of Ns (number of grid points for each parameter) of the brute algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25292b95-32d9-47b9-9867-2f1eab26998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the data\n",
    "p     = 5      # Participant 5\n",
    "t_max = 720    # the first 20 trials\n",
    "C     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy()\n",
    "X     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy()\n",
    "T     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy()\n",
    "\n",
    "\n",
    "param_names  = list( rand_parameter.keys() )\n",
    "EPS = 0.01   #used to avoid side effects \n",
    "\n",
    "#################################\n",
    "# Function to be minimized. To be compatible with the optimization method, \n",
    "# the function must be in the form f( param_values, *args), where param_values is a 1-D array of parameter values.\n",
    "# In our context, args contains the arguments of log_likelihood (model, X, C, T)\n",
    "# it requires also an additional argument param_names to maintain the link between parameter name et parameter values\n",
    "# so len( param_values ) == len( param_values )\n",
    "# return the inverse of log likelihood\n",
    "#################################\n",
    "def inv_log_likelihood( param_values, param_names, model, X, C, T ):\n",
    "    ######################\n",
    "    #TODO 1\n",
    "    # param = ... \n",
    "    ######################\n",
    "    return - log_likelihood( model, param, X, C, T )[0]\n",
    "\n",
    "\n",
    "##################################\n",
    "#              manual\n",
    "##################################\n",
    "print( \"Method: Manual \\t \\t \\t log_likelihood:\", manual_ll, \"\\t theta:\", manual_theta, \"\\t elasped time:\", manual_elapsed_time ) \n",
    "\n",
    "\n",
    "t = time.process_time()\n",
    "\n",
    "#####################\n",
    "# TODO 2 (optimisation method: Brute force)\n",
    "# scipy.optimize.brute(...)\n",
    "# use the following arguments: func = ..., ranges = ..., args =..., Ns = 20, full_output=True, finish= None\n",
    "# res = brute(...)\n",
    "#####################\n",
    "\n",
    "elapsed_time = time.process_time() - t\n",
    "print( \"Method: Brute (Grid search) \\t log_likelihood:\", - round( res[1], 3), \"\\t theta:\", round( res[0], 3 ), \"\\t elasped time:\", round( elapsed_time, 3 ) ) \n",
    "\n",
    " \n",
    "#################################\n",
    "#    differential evolution\n",
    "#################################\n",
    "t = time.process_time()\n",
    "\n",
    "#####################\n",
    "# TODO 2 (optimisation method: Differential evolution)\n",
    "# scipy.optimize.differential_evolution\n",
    "# use the following arguments: func = ..., bounds = ..., args =...\n",
    "# Notice that the ``bounds'' argument of differential_evolution is the equivalent to the ``ranges'' argument of brute\n",
    "# res = differential_evolution(...)\n",
    "##################### \n",
    "\n",
    "elapsed_time = time.process_time() - t\n",
    "print( \"Method: Differential evolution \\t log_likelihood:\", - round( res.fun, 3), \"\\t theta:\", round( res.x[ 0 ], 3 ),  \"\\t elasped time:\", round( elapsed_time, 3 ) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341a49e-5a80-4d49-b666-3e81c0dfa17b",
   "metadata": {},
   "source": [
    "#### 3.7 Conclusion\n",
    "You are now able to compute the best parameter(s) and the log likelihood for a given model. While this is a very simple model with only one parameter, you can see the impact of the optimization method (and calibration) on parameter fits. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "⚠️ You should be careful regarding <strong>Computational Solving</strong>. For some complex models with multiple parameters, you have sometimes to make a compromise between model plausability and computational solving.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e11243-8ed1-49f5-93d3-407e6e501eae",
   "metadata": {},
   "source": [
    "# Part 4: Model comparison and selection (45mn)\n",
    "<ol>\n",
    "    <li> Soft max function </li>\n",
    "    <li> Choice Kernel (CK) </li>\n",
    "    <li> Rescorla-Wagner (RW) </li>\n",
    "    <li> Comparing models with the log likelihood </li>\n",
    "    <li> Comparing models with BIC score</li>\n",
    "    <li> BIC score and significance</li>\n",
    "    <li> Summary: reporting data </li>\n",
    "</ol>\n",
    "\n",
    "To compare models, we first need ... a set of candidate models. Let consider three additional basic models in computational neurosciences. But before that, let me introduce the softmax function which is useful for the following models.\n",
    "\n",
    "#### 4.1 Soft max function\n",
    "The softmax function converts a vector  $V$ of $k$ real numbers $V={v^1, v^2, ... v^k}$ into a probability distribution of k possible outcomes.\n",
    "\n",
    "$$ P(v^i) = \\frac{ exp( \\quad \\beta \\times v^i \\quad ) }{ \\sum_{i=1}^k exp( \\quad\\beta \\times v^i \\quad ) } $$\n",
    "\n",
    "where $\\beta$ is the inverse temperature which is the level of stochasticity. $\\beta = 0$ means that the choice is random. $\\beta = \\infty$ means the chosen option is the one with the largest value. Below the implementation of the softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67005971-82f0-480f-ace1-737b0ad5de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#                               Softmax                                  #\n",
    "# Input:                                                                 #\n",
    "#    - beta (float) : inverse temperature                                #\n",
    "#    - values (np.array): list containing the values for each action     #\n",
    "# Output:                                                                #\n",
    "#    - probs (np.array) : the probability of each action to be chosen    #\n",
    "########################################################################## \n",
    "MAX_EXP_FLOAT = np.log(sys.float_info.max)\n",
    "\n",
    "def soft_max(beta, values):\n",
    "    values = beta * values\n",
    "    values = np.where( values < MAX_EXP_FLOAT, values, MAX_EXP_FLOAT) \n",
    "    return np.exp( values ) / max( np.sum( np.exp( values ), axis = 0 ), EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f93de-ba18-45a8-8187-6c4f91e5bf05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> if you are not familiar with softmax, you can test different value_vec and beta_vec and visualize the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bd467-3988-4fd7-bef9-74ede84c7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#TODO test soft_max\n",
    "#######################\n",
    "value_vec = np.array( [0.1, 0.4, 0.5] )\n",
    "beta_vec  = np.array( [0, 0.5, 2, 5, 10, 20] )\n",
    "print( \"Test soft_max with the values\",  value_vec, \"for different values of beta\" )\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 4.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "f0 = plt.figure( figsize=(16, 2) )\n",
    "f, axes = plt.subplots(1, len(beta_vec) +1)\n",
    "axes[0].bar( [\"M\", \"L\", \"S\"], value_vec )\n",
    "axes[0].set( title=\"Values\", ylim=[0,1])\n",
    "for i in range(0, len(beta_vec) ) :\n",
    "    axes[i+1].bar( [\"M\", \"L\", \"S\"], soft_max( beta_vec[i], value_vec ) )\n",
    "    axes[i+1].set( title=\"beta=\" + str(beta_vec[i]), ylabel= \"probability distribution\", ylim=[0,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b66f4-162b-4abb-9b6e-30bfcdc89a07",
   "metadata": {},
   "source": [
    "Information shared to all models:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3494d9-c203-4d27-bbb1-33a5e9c4a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_command  = 14 # Number of commands\n",
    "n_strategy = 3  # Number of strategies / actions (MENU, SHORTCUT, LEARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f41eb-5a1c-4f26-a554-efb1dda8decd",
   "metadata": {},
   "source": [
    "#### 4.2 Choice Kernel (CK) model. \n",
    "This model, often used in neuroscience, **captures the user's tendency to repeat previous actions**. The user calculates a \"choice kernel\" $CK_t^k(c)$ that remembers the frequency of use of the strategy (or action) $k$ for the command $c$. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$ CK_{t+1}^k(c) =  CK_t^k(c) + \\alpha_{CK} (a_t^k - CK_t^k(c) )$$ \n",
    "\n",
    "where $a_t^k = 1$ if strategy $k$ is used in trial $t$, otherwise $a_t^k = 0$ and $ \\alpha_{CK}$ is the learning rate.\n",
    "</div>\n",
    "\n",
    "To simplify, we consider that the initial value of the kernel choice is 0. We can then calculate the probability of executing the strategy $a$ for the command $c$ using the softmax function:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$ P(c, a^k) = \\frac{ exp( \\quad \\beta_{CK} \\times CK_t^k(c) \\quad ) }{ \\sum_{i=0}^2 exp( \\quad\\beta_{CK} \\times CK_t^i(c) \\quad ) } $$\n",
    "where $\\beta_{CK}$ is the inverse temperature defining the level of stochasticity. \n",
    "</div>\n",
    "<strong>This model has two parameters ($\\alpha_{CK}$, $\\beta_{CK}$)</strong>\n",
    "\n",
    "A key difference with the previous model is that <strong>the output of the model now depends on the CK values which are updated at each trial</strong>, ie. the output depends on the sequence of previous users choices.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> Implement the CK model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914eeef3-1636-4ead-a39a-4ba8eec166af",
   "metadata": {},
   "outputs": [],
   "source": [
    "CK_value      =  np.zeros( (n_command, n_strategy) )\n",
    "CK_parameter  = { 'A_CK' : 0.2, 'B_CK': 2 }\n",
    "        \n",
    "#######################\n",
    "def CK_action_probability( parameter, command ) :\n",
    "    beta  = parameter[ 'B_CK' ]\n",
    "    ##################\n",
    "    # TODO\n",
    "    # return ...\n",
    "    ##################\n",
    "\n",
    "#######################\n",
    "def CK_update( parameter, action, command, time ) :\n",
    "    global CK_value\n",
    "    alpha = parameter[ 'A_CK']\n",
    "    ###################\n",
    "    # TODO\n",
    "    # do not forget to update the values of the three strategies \n",
    "    # CK_value [ command ] = ...\n",
    "    ###################\n",
    "\n",
    "#######################\n",
    "def CK_reset() :\n",
    "    global CK_value\n",
    "    CK_value = np.zeros( (n_command, n_strategy) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef10b1-c638-49bf-b6bf-708ee8419f7c",
   "metadata": {},
   "source": [
    "#### 4.3 Rescorla-Wagner (RW) Model\n",
    "\n",
    "This model learns the $Q_t^k$ value associated with each strategy/action $k$ and each command $c$ based on the history of the actions and the associated gains. The model then uses these values to decide which action to perform:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$ Q_{t+1}^k(c) = Q_{t}^k(c) + \\alpha_{RW} \\times (r_t - Q_{t}^k(c)) $$\n",
    "\n",
    "where $r_t$ is the reward at the trial $t$ and $\\alpha_{RW}$  is the learning rate between 0 and 1. \n",
    "</div>\n",
    "We can then estimate the probability to choose $a^k$ for the command $c$ according to the softmax\n",
    "function as the Choice Kernel model:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$ P(c, a^k) = \\frac{ exp( \\quad \\beta_{RW} \\times Q_t^k(c) \\quad ) }{ \\sum_{i=0}^2 exp( \\quad\\beta_{RW} \\times Q_t^i(c) \\quad ) } $$\n",
    "\n",
    "where $\\beta_{RW}$ is the inverse temperature defining the level of stochasticity. \n",
    "</div>\n",
    "<strong>This model has two parameters ($\\alpha_{RW}$, $\\beta_{RW}$)</strong>.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Implement the RW model considering that the reward is $r = max(time) - time$. $time$ is the time to execute a command and $max( time ) = 10$ was empirically defined. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417ed55-80f5-4352-a5a7-203cd45662da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 10.0\n",
    "Q_value = np.zeros( (n_command, n_strategy) )  \n",
    "RW_parameter  = { 'A_RW' : 0.2, 'B_RW' : 1 }\n",
    "        \n",
    "#######################\n",
    "def RW_action_probability( parameter, command ) :\n",
    "    beta  = parameter[ 'B_RW' ]\n",
    "    ################\n",
    "    # TODO\n",
    "    # return ...\n",
    "    ################\n",
    "\n",
    "#######################\n",
    "def RW_update( parameter, action, command, time ) :\n",
    "    alpha = parameter[ 'A_RW' ]\n",
    "    global Q_value\n",
    "    #################\n",
    "    # TODO\n",
    "    # Q_value[ command ][...]\n",
    "    #################\n",
    "\n",
    "#######################\n",
    "def RW_reset() :\n",
    "    global Q_value\n",
    "    Q_value = np.zeros( (n_command, n_strategy) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630fc94-00b7-4670-96d8-16a8406bd3bb",
   "metadata": {},
   "source": [
    "#### 4.4 Rescorla-Wagner + Choice Kernel\n",
    "This model mixes the two previous models. The previous equations will be used to update the internal variables. The probability of choosing an action is now defined as:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "$$ P(c, a^k) = \\frac{ exp( \\quad \\beta_{CK} \\times CK_t^k(c) + \\beta_{RW} \\times Q_t^k(c) \\quad ) }{ \\sum_{i=0}^2 exp( \\quad \\beta_{CK} \\times CK_t^i(c) + \\beta_{RW} \\times Q_t^i(c) \\quad ) } $$\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Implement the RWCK model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26972b4f-aa66-40c9-9c50-7dfb84b49e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "RWCK_value = { 'CK' : np.zeros( (n_command, n_strategy) ), 'Q' : np.zeros( (n_command, n_strategy) ) } \n",
    "RWCK_parameter      = { 'A_RW' : 0.2, 'B_RW' : 2, 'A_CK' : 0.2, 'B_CK' : 2 }\n",
    "\n",
    "#######################\n",
    "def double_soft_max( beta_1, values_1, beta_2, values_2 ):\n",
    "    values = beta_1 * values_1 + beta_2 * values_2\n",
    "    values = np.where( values < MAX_EXP_FLOAT, values, MAX_EXP_FLOAT)\n",
    "    return np.exp( values ) / max( np.sum( (np.exp( values) ), axis = 0), EPS )\n",
    "\n",
    "#######################\n",
    "def RWCK_action_probability( parameter, command ) :\n",
    "    beta_CK  = parameter[ 'B_CK' ]\n",
    "    beta_RW  = parameter[ 'B_RW' ]\n",
    "\n",
    "    ################\n",
    "    #TODO\n",
    "    #return ...\n",
    "    ################\n",
    "\n",
    "#######################\n",
    "def RWCK_update( parameter, action, command, time ) :\n",
    "    global RWCK_value\n",
    "    alpha_CK = parameter[ 'A_CK' ]\n",
    "    alpha_RW = parameter[ 'A_RW' ]\n",
    "\n",
    "    #################\n",
    "    #TODO\n",
    "    # RWCK_value[ 'Q' ][...][...]  =\n",
    "    # RWCK_value[ 'CK' ][...] = \n",
    "    #################\n",
    "\n",
    "#######################\n",
    "def RWCK_reset() :\n",
    "    global RWCK_value\n",
    "    RWCK_value[ 'CK' ] = np.zeros( (n_command, n_strategy) )\n",
    "    RWCK_value[ 'Q' ]  = np.zeros( (n_command, n_strategy) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd102c42-42e0-4ad3-8da8-ad087f514b38",
   "metadata": {},
   "source": [
    "We now have four models : \n",
    "- Random\n",
    "- Choice Kernel (CK)\n",
    "- Rescorla-Wagner (RW)\n",
    "- Rescorla-Wager and Choice Kernel (RWCK)\n",
    "\n",
    "To compare the four models, we need the same data structure. We will thus use a class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db656275-1804-4488-91ff-686d7bfbc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object) : \n",
    "    def __init__( self, name, action_probability, update, reset, parameter, bounds ):\n",
    "        self.name = name\n",
    "        self.action_probability = action_probability\n",
    "        self.update = update\n",
    "        self.reset  = reset\n",
    "        self.parameter = parameter # dict \n",
    "        self.bounds    = bounds    # dict\n",
    "\n",
    "    # return an array of the parameter names, necessary for the optimization methods\n",
    "    def parameter_names( self ) :\n",
    "        return list( self.parameter.keys() )\n",
    "        \n",
    "    # return an array of the parameter bounds, necessary for the optimization methods\n",
    "    def parameter_bounds( self ) :\n",
    "        return list( self.bounds.values() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3b94b-47fd-4b5d-b9e7-6ec4e9913b59",
   "metadata": {},
   "source": [
    "For the random Model, we already implemented *rand_action_probability()*, but not *rand_update()* and *rand_reset()*. Let's define them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de36da3-3c6d-4415-b3e7-2d8e754b2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# get the same signature for all models\n",
    "# update the signature of the random model\n",
    "def rand_update( parameter, action, command, time ) :\n",
    "    return\n",
    "\n",
    "def rand_reset() :\n",
    "    return\n",
    "##############\n",
    "max_beta = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b2005-c36e-417d-945a-3e43f946bf88",
   "metadata": {},
   "source": [
    "For each model, we define the parameter bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697df0a-c15b-4283-a1e2-f74a13d648f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounds of all models\n",
    "rand_bounds = { 'THETA': [EPS, 1-EPS] }\n",
    "CK_bounds   = { 'A_CK' : [EPS, 1-EPS], 'B_CK' : [0, max_beta] }\n",
    "RW_bounds   = { 'A_RW' : [EPS, 1-EPS], 'B_RW' : [0, max_beta] }\n",
    "RWCK_bounds = { 'A_RW' : [EPS, 1-EPS], 'B_RW' : [0, max_beta], 'A_CK' : [EPS, 1-EPS], 'B_CK' : [0, max_beta] }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e2cd9-e65e-459d-9577-becbee385419",
   "metadata": {},
   "source": [
    "We can now create our vector with our four models: model_vec contains the four models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e551c-27f3-4040-952e-ea68a914d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rand_model = Model( 'Random', rand_action_probability, rand_update, rand_reset, rand_parameter, rand_bounds )\n",
    "CK_model   = Model( 'CK'    , CK_action_probability  , CK_update  , CK_reset  , CK_parameter  , CK_bounds   ) \n",
    "RW_model   = Model( 'RW'    , RW_action_probability  , RW_update  , RW_reset  , RW_parameter  , RW_bounds   )\n",
    "RWCK_model = Model('RWCK'   , RWCK_action_probability, RWCK_update, RWCK_reset, RWCK_parameter, RWCK_bounds )\n",
    "model_vec = [ Rand_model, CK_model, RW_model, RWCK_model ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996e8cd-af31-448b-871c-718507c89860",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> Display a summary of the four models, indicating for each model, the list of the parameter names and the corresponding parameter bounds.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f987da0-9463-4599-94d0-179e1ab221f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_vec :\n",
    "    ############\n",
    "    # TODO \n",
    "    # print( ... )\n",
    "    print( model.bounds )\n",
    "    #############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54b1e-38ab-4d5f-9229-1fca50607e6e",
   "metadata": {},
   "source": [
    "#### 4.5 Model comparison with the log-likelihood\n",
    "\n",
    "We are almost ready to compare the log_likelihood of these models, but we first need to refine the log_likelihood function. Indeed, the likelihood of an observation at the trial $t$ depdends on the sequence of observations of the <strong>participant from the trial 0 to the trial t-1</strong>.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> Implement the novel log-likelihood function using the class <i>Model</i> and the methods <i>action_probability</i>, <i>reset</i> and <i>update</i>. Moreover, we will take care that the probabilities are never equal to 0 ($P(x) > EPS)$ )\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea2113-58f6-4a6f-98dd-58ad6f2a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# m:           Model, \n",
    "# parameter :  set of parameters of the model m \n",
    "# X:           sequence of observations\n",
    "# C:           seqeunce of commands\n",
    "# T:           sequence of time\n",
    "#################################\n",
    "def log_likelihood( m, parameter, X, C, T):\n",
    "    prob_vec = np.ones( len(X) )\n",
    "    ###########\n",
    "    # TODO\n",
    "    ###########\n",
    "    return np.sum( np.log( prob_vec ) ), prob_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0352f9-2f13-423c-addb-65f53dcf79e2",
   "metadata": {},
   "source": [
    "We are now ready to compare our models\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Compare the log-likelihood (ll) of the four models using your favorite optimization method. Do not forget that the optimization should call inv_log_likelihood!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace9c41-3d6f-48fa-97b6-d4767055d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the data\n",
    "p     = 5      # Participant 5\n",
    "t_max = 720    \n",
    "X     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy()\n",
    "C     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy()\n",
    "T     = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy()\n",
    "\n",
    "result_df = pd.DataFrame([], columns=['participant', 'model', 'parameters', 'log likelihood'] )\n",
    "\n",
    "for model in model_vec :\n",
    "    param_names = model.parameter_names()\n",
    "    bounds      = model.parameter_bounds()\n",
    "    t   = time.process_time()\n",
    "    #############\n",
    "    # TODO\n",
    "    # res = differential_evolution(...)\n",
    "    # ll  = ... (max log-likelihood)\n",
    "    ##############\n",
    "    elapsed_time = time.process_time() - t    \n",
    "\n",
    "    # best parameters\n",
    "    best_parameters = str( dict( zip(param_names, np.round( res.x, 2 ) ) ) )\n",
    "    result_df.loc[ len(result_df.index) ] = [ p, model.name, best_parameters, ll ]\n",
    "    print(\"parameter fit:\", model.name, '\\t [Done]')\n",
    "\n",
    "print( \"finished\" )\n",
    "\n",
    "print( result_df[ ['model', 'log likelihood' ] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8049d-08ca-4a81-9980-7e59f2bd6f21",
   "metadata": {},
   "source": [
    "We can now visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae72a2-6cb3-4859-879a-23516a5f6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot model comparison\n",
    "plt.rcParams[\"figure.figsize\"] = [6.00, 4.0]\n",
    "rel = sns.barplot( data = result_df, x = 'model', y = - result_df[ 'log likelihood' ] )\n",
    "f = rel.set( title= 'Model comparison (log-likelihood)', xlabel = \"Model\", ylabel= r'$-log\\mathcal{L}$' )\n",
    "for i in rel.containers:\n",
    "    rel.bar_label(i,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df547860-0da0-4eb2-b103-a0299c930963",
   "metadata": {},
   "source": [
    "It is common to report this figure in article. In particular to show $-log\\mathcal{L}$ rather than $log\\mathcal{L}$ because $log\\mathcal{L}$ is negative. The smaller the better. So, which model best describes the data according to the log likelihood?\n",
    "\n",
    "We can also have a look at the values of the optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c6a8-ca97-44b9-b085-077681a5e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( result_df[ [ 'model', 'parameters' ] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a1983-29c1-4f08-a6e6-897eb385046c",
   "metadata": {},
   "source": [
    "#### 4.6 Model comparison with the BIC score\n",
    "RWCK best describes the data but... it is also the most \"complex\" model, ie. the model with the largest number of parameters. Number of parameters per model :\n",
    "- Random: 1\n",
    "- CK: 2\n",
    "- RW: 2\n",
    "- RWCK: 4\n",
    "\n",
    "A limitation of the log likelihood is to favor complex models. Let consider an extreme case, a model with 720 parameters, one paremeter for each observation where the parameter value equals to the participant strategy. With the current parameter fitting procedure, the log likelihood would be perfect, $log\\mathcal{L} = 0$. Is it a \"good\" model in term of explaining human behaviour? clearly not. \n",
    "\n",
    "The Bayesian Information Criterion (BIC) score addresses this problem, because it is often used to penalize models with many parameters:\n",
    "$$ BIC = ln(n) * k - 2 * log\\mathcal{L}(m) $$\n",
    "\n",
    "where \n",
    "- $n$ is the number of observations to be predicted (here $n=720$),\n",
    "- $k$ is the number of model parameters and\n",
    "- $\\mathcal{L}(m)$ the model log likelihood.\n",
    "\n",
    "**The BIC score is positive and the closer to zero, the better**.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong> TODO: </strong> Implement the BIC score function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20b2c0-0100-48d9-99b5-17abfd34b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Estimate the BIC Score                       #\n",
    "# Input:                                       #\n",
    "#    - n : number of observations to predict   #\n",
    "#    - k : number of parameters of the model   #\n",
    "#    - ll: log likelihood                      #\n",
    "# Ouptput:                                     #\n",
    "#    -  n * k - 2 ll                           #\n",
    "################################################\n",
    "def bic_score( n, k, ll ):\n",
    "    #########\n",
    "    # TODO\n",
    "    # return ...\n",
    "    #########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9ed17-e6c8-4603-8888-8c552c9104b7",
   "metadata": {},
   "source": [
    "We can now compare the models according to the BIC Score. We Update the dataframe *result_df* which is the results of parameter fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642b583-423a-43d6-ad5c-6bac7889bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[ 'k' ]  = 0\n",
    "result_df[ 'BIC'] = 0\n",
    "result_df[ 'n']   = t_max # number of trials\n",
    "print( result_df )\n",
    "\n",
    "for model in model_vec :\n",
    "    # update k\n",
    "    result_df.loc[ result_df.model == model.name, 'k' ] = len( model.parameter_names() )\n",
    "    \n",
    "    # estimate BIC\n",
    "    result_df[ 'BIC' ] = round( bic_score( t_max, result_df[ 'k' ], result_df[ 'log likelihood' ] ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f353ce-5886-4e34-8ec8-850477a02ba3",
   "metadata": {},
   "source": [
    "We can now visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41c9ef-2410-47a9-b225-4df26b48d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot BIC score comparison\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 6.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "f, axes = plt.subplots(1, 2)\n",
    "rel0 = sns.barplot( data = result_df, x = 'model', y = - result_df[ 'log likelihood' ], ax = axes[0] )\n",
    "f = rel0.set( title= 'Model comparison (log likelihood)', xlabel = \"Model\", ylabel= r'$-log\\mathcal{L}$' )\n",
    "for i in rel0.containers:\n",
    "    rel0.bar_label(i,)\n",
    "\n",
    "rel1 = sns.barplot( data = result_df, x = 'model', y = result_df[ 'BIC' ], ax = axes[1] )\n",
    "t = rel1.set( title= 'Model comparison (BIC score)', xlabel = \"Model\", ylabel= 'BIC' )\n",
    "for i in rel1.containers:\n",
    "    rel1.bar_label(i,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1c030-e2a8-45a9-b3d4-9b726a76dc85",
   "metadata": {},
   "source": [
    "What do you observe? Which is the best model according to the BIC score?\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> Let see what's happen for the participant 10. Go back to 4.5 and change participant = 5 to participant = 10. What do you see? Why?\n",
    "</div>\n",
    "#### 4.7 BIC score and significance\n",
    "Can we say that CK is significantly better than RWCK? It is common practice to consider that there is a “strong evidence\" in favor of the winning model when the BIC difference is > 6 [Raftery. 1995].\n",
    "\n",
    "\n",
    "Here, an example how we can report the data:\n",
    "<div style=\"margin-left:200px;margin-right:200px; \"> \n",
    "<i>According to the results, there are strong evidence (BIC difference > 6) that model A ($log\\mathcal{L} = 386.4$, $BIC = 786.0$) outperforms the model B ($log\\mathcal{L} = 383.8$, $BIC = 794.0$).</i>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### 4.8 Individual vs. Population\n",
    "Until now, we only considered the participant P5 (and P10). The reason is two-fold: (1) start simple, (2) save time as the optimization methods take time. We now consider now a **population** with 5 participants (P2, P4, P5, P10, P11). If it takes too much time, you can use the file \"save_result_df.csv\" at the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65cb49-f4c9-4bbf-a807-30e37ddfbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = np.array( [2,4,5,10,11] ) # list of 5 participants\n",
    "t_max        = 720                       # all trials\n",
    "result_df    = pd.DataFrame([], columns=['participant', 'model', 'k', 'parameter names', 'parameter values', 'log likelihood', 'BIC'] )\n",
    "\n",
    "for p in participants :\n",
    "    C  = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'cmd_input' ].to_numpy()\n",
    "    X  = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'strategy'  ].to_numpy()\n",
    "    T  = data.loc[ (data.participant == p) & (data.trial_id < t_max), 'time'      ].to_numpy()\n",
    "    \n",
    "    print (\"====== Participant: \", p , \"======\" )\n",
    "    for model in model_vec :\n",
    "        param_names  = model.parameter_names()\n",
    "        k            = len( param_names )\n",
    "        bounds       = model.parameter_bounds()\n",
    "        \n",
    "        res = differential_evolution( inv_log_likelihood, bounds = bounds, maxiter=500, args = (param_names, model, X, C, T) )\n",
    "          \n",
    "        ll  = - round( res.fun, 1)\n",
    "        bic = round( bic_score( t_max, k, ll ), 1)\n",
    "        best_parameters = str( dict( zip(param_names, np.round( res.x, 2 ) ) ) )\n",
    "        print( model.name, \"BIC score:\", bic, \"log L:\", ll )\n",
    "        print( best_parameters )\n",
    "        result_df.loc[ len(result_df.index) ] = [ p, model.name, k, \",\".join( model.parameter_names() ), best_parameters, ll, bic  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca70fcb-9cf6-4278-ab23-96c2beba3d54",
   "metadata": {},
   "source": [
    "You can save / load these resultas (result_df dataframe) to reuse it wiwhout having to relaunch the optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018400d-000d-44d7-ac48-e3e5346f3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result_df (use your own filename)\n",
    "#result_df.to_csv( './data/saved_result_df.csv' )\n",
    "\n",
    "# load result_df\n",
    "#result_df = pd.read_csv( './data/saved_result_df.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3aa479-590e-41e6-a16b-ab6bfee6d2aa",
   "metadata": {},
   "source": [
    "We can now compare models for the whole population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15f9df-257e-4b75-acc5-6717410b300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 6.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "def display_population_results( result_df ) :\n",
    "    n_models = len( result_df[ 'model' ].unique() )\n",
    "    display_df = result_df.copy()\n",
    "    f, axes = plt.subplots(1, 2)\n",
    "    order = ['Random', 'CK', 'RW', 'RWCK' ]\n",
    "    model_df = display_df.groupby(['model']).mean()\n",
    "    model_df = model_df.reset_index()\n",
    "    model_df['model'] = pd.Categorical(model_df['model'], order )\n",
    "    model_df = model_df.sort_values( 'model' )\n",
    "    \n",
    "    #edge color\n",
    "    edges_ll = np.array( ['white'] * len( model_df[ 'model' ] ) ) \n",
    "    edges_ll[ np.argmax( model_df[ 'log likelihood' ] ) ] = 'black'\n",
    "    \n",
    "    rel0 = sns.barplot( data = display_df, x = 'model', y = - np.round( display_df[ 'log likelihood' ], 1), order = order, edgecolor = edges_ll, linewidth = 2,ax = axes[0] )\n",
    "    \n",
    "    f = rel0.set( title= 'Model comparison (log likelihood) at the population level', xlabel = \"Model\", ylabel= r'$-log\\mathcal{L}$' )\n",
    "    for i in rel0.containers:\n",
    "        rel0.bar_label(i,)\n",
    "    \n",
    "    #edge color\n",
    "    edges_BIC = np.array( ['white'] * len( model_df[ 'model' ] ) ) \n",
    "    edges_BIC[ np.argmin( model_df[ 'BIC' ] ) ] = 'black'\n",
    "    rel1 = sns.barplot( data = display_df, x = 'model', y = np.round(display_df[ 'BIC' ], 1), order = order, edgecolor = edges_BIC, linewidth = 2, ax = axes[1] )\n",
    "    t = rel1.set( title= 'Model comparison (BIC score) at the population level', xlabel = \"Model\", ylabel= 'BIC' )\n",
    "    for i in rel1.containers:\n",
    "        rel1.bar_label(i,)\n",
    "    plt.show()\n",
    "\n",
    "display_population_results( result_df )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b038838-1e82-4ad8-a979-743fa146374b",
   "metadata": {},
   "source": [
    "The figure compares the four models in term of $log\\mathcal{L}$ (left) and BIC score (right) for the whole population. The lower the better. The black outline indicates the winning model. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Which is the winning model for the whole <strong>population</strong>?\n",
    "    \n",
    "Do we have the same results for each participant / <strong>individual</strong>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228d7c7-3335-4ce9-90be-517457af00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_individual_results( result_df ) :\n",
    "    n_models = len( result_df[ 'model' ].unique() )\n",
    "    display_df = result_df.copy()\n",
    "    f, axes = plt.subplots(1, 2)\n",
    "    \n",
    "    display_df[ 'max_ll_tmp'  ]  = display_df.groupby(['participant'])['log likelihood'].transform(max)\n",
    "    display_df[ 'max_ll' ]  = 0\n",
    "    # - log likelyhood for the winning model, 0 otherwise\n",
    "    display_df.loc[ display_df[ 'log likelihood' ] == display_df[ 'max_ll_tmp' ], 'max_ll' ] = - display_df[ 'max_ll_tmp'  ]\n",
    "\n",
    "    #draw likelihood for all models\n",
    "    rel0 = sns.barplot( data = display_df, x = 'participant', y = - display_df[ 'log likelihood' ], hue=\"model\", edgecolor = 'white', linewidth = 2, ax = axes[0] )\n",
    "    #draw likelihood for the winning model\n",
    "    sns.barplot(        data = display_df, x = 'participant', y = 'max_ll', hue= \"model\", edgecolor = 'black', linewidth = 2, ax = axes[0] )\n",
    "\n",
    "    #title, labels and legend\n",
    "    f = rel0.set( title= 'Model comparison (log likelihood) for each individual', xlabel = \"Model\", ylabel= r'$-log\\mathcal{L}$' )\n",
    "    h, l = axes[0].get_legend_handles_labels()\n",
    "    axes[0].legend(h[0:n_models], l[0:n_models])\n",
    "    \n",
    "    \n",
    "    display_df['min_BIC_tmp'] = display_df.groupby(['participant'])['BIC'].transform(min)\n",
    "    display_df[ 'min_BIC' ]  = 0\n",
    "    # bic for the winning model, 0 otherwise\n",
    "    display_df.loc[ display_df[ 'BIC' ] == display_df[ 'min_BIC_tmp' ], 'min_BIC' ] = display_df[ 'min_BIC_tmp'  ]\n",
    "    \n",
    "    #draw BIC for all models\n",
    "    rel0 = sns.barplot( data = display_df, x = 'participant', y = display_df[ 'BIC' ], hue=\"model\", edgecolor = 'white', linewidth = 2, ax = axes[1] )\n",
    "    #draw BIC for the winning model\n",
    "    sns.barplot(        data = display_df, x = 'participant', y = 'min_BIC', hue= \"model\", edgecolor = 'black', linewidth = 2, ax = axes[1] )\n",
    "\n",
    "    #title, labels and legend\n",
    "    f = rel0.set( title= 'Model comparison (BIC) for each individual', xlabel = \"Model\", ylabel= 'BIC score' )\n",
    "    h, l = axes[1].get_legend_handles_labels()\n",
    "    axes[1].legend(h[0:n_models], l[0:n_models])\n",
    "\n",
    "display_individual_results( result_df )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0a984-2648-4d18-99b2-d70401bf3774",
   "metadata": {},
   "source": [
    "Comparing models for each individual reveals another story. There is no a single model for all participants.\n",
    "\n",
    "Two different individuals can have different strategies and thus behaviors. One model can better describes one strategy / behavior while another model can better describes another strategy / behavior. Here, for instance, P10 never learned or used shortcut. A simple biased random model is sufficient to describe this behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673d226-a140-43b4-ba35-739ee0e1e0d7",
   "metadata": {},
   "source": [
    "#### 4.9 Summary and reporting results\n",
    "In your article, it is good practice to include the two last figures ($log\\mathcal{L}$ and BIC both at the population and individual level) as well as the following table (latex code generated) summarizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685d06a-48d1-4a76-be4a-efeccb464369",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "##################################\n",
    "def model_comparison_table( result_df) :\n",
    "    ll = '- log L'\n",
    "    order    = ['Random', 'CK', 'RW', 'RWCK' ]\n",
    "    model_df = result_df.groupby(['model', 'parameter names' ]).mean()\n",
    "    model_df = model_df.reset_index()\n",
    "    model_df['model'] = pd.Categorical(model_df['model'], order )\n",
    "    model_df = model_df.sort_values( 'model' )\n",
    "    model_df[ 'participant' ] = model_df[ 'participant' ].astype(int)\n",
    "    model_df[ 'k' ] = model_df[ 'k' ].astype(int)\n",
    "    model_df.rename(columns={'log likelihood': ll, 'parameter names':'parameters' }, inplace=True)\n",
    "    model_df[ ll ] = - np.round( model_df[ ll ] , 1 )\n",
    "    model_df[ 'BIC' ] = np.round( model_df[ 'BIC' ] , 1 )\n",
    "\n",
    "    table = model_df[ ['model', 'k', 'parameters', ll , 'BIC' ] ]\n",
    "    \n",
    "    print( \"--------------------------------------------------------------------------\" )\n",
    "    print( table.to_string(index=False) )\n",
    "    print( \"--------------------------------------------------------------------------\" )\n",
    "\n",
    "    print( table.to_latex() )\n",
    "\n",
    "model_comparison_table( result_df )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c3a71-e6e6-49bb-b83e-76a54dd70103",
   "metadata": {},
   "source": [
    "# Part 5: Model Simulation (30mn)\n",
    "\n",
    "Now that we have the best set of parameters for each model, we can simulate them. In some cases, model simulation can lead to very different results from model fitting if the path of actions sampled by the participant is widely different from the paths likely to be selected by the model. <strong> It is thus important to also simulate the models </strong> and verify that they do reproduce the main behavioral properties of the participants (wilson and Collins). In our context, it can be the evolution of the percentage of correct shortcut execution, which is commonly used to compare interaction techniques favoring shortcuts, i.e. how well the models can reproduce the first figure of the exercice. \n",
    "\n",
    "\n",
    "<ol>\n",
    "<li> Simulation preparation </li>\n",
    "<li> Simulation implementation</li>\n",
    "<li>  Simulation visualization </li>\n",
    "<li> Mean Square error (MSE) </li>\n",
    "<li> Conclusion </li>\n",
    "</ol>\n",
    "\n",
    "#### 5.1 Simulation preparation\n",
    "The results of our simulation are saved in a dataframe similar to the one containing the empirical data of the participant, except that it requires an additional column indicating the id of the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99446c34-0b12-40fe-ae1e-416e6d73b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = np.array( [2,4,5,11] ) # list of 5 participants\n",
    "all_df = pd.read_csv( './data/audio_hotkey.csv' )\n",
    "all_df = all_df[ all_df[ 'participant' ].isin( participants ) ]\n",
    "all_df[ 'sim id' ]   = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e2505-1370-447f-b814-3dc403ecfefc",
   "metadata": {},
   "source": [
    "To run a simulation, the model should generate at each time step: \n",
    "- an action,\n",
    "- its duration. To simplify, we assume execution time only depends on the strategy: $T_{Menu} = 2.0$, $T_{SHORTCUT}= 0.9$, $T_{LEARNING} = 3.8$.\n",
    "- whether the user executed the righ command. To simplify, we assume no error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9078369-e7c1-41cc-8f54-2dd40e51d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################\n",
    "def time( model, action) : \n",
    "    t = np.array( [2.0, 0.9, 3.8] ) # model and history independent\n",
    "    return t[ action ]\n",
    "    \n",
    "####################    \n",
    "def success_probability( model, action ) :\n",
    "    return 1.0                      # no error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360307fe-71a4-4915-a77b-08cac642696a",
   "metadata": {},
   "source": [
    "#### Util\n",
    "In section 4, the results of the parameter fits procedure was saved in the dataframe **result_df** (see above). Here a function to assign the best parameter value to a model from this data frame. The best parameter depends on the participant. This function is necessary when running simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf33d33-284a-4cbb-b84e-215501353e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# assign the best parameters for a given participant to the model \n",
    "# based on the results of the parameter fits procedure (saved in result_df)\n",
    "# - model      : an instance of the Model class\n",
    "# - participant: participant id\n",
    "# - fit_results:  a dataframe with a at least three columns: \n",
    "#                 - 'name' (name of the model), \n",
    "#                 - 'participant' (participant id), \n",
    "#                 - 'parameter values' (dict containing the best parameter values) \n",
    "def set_best_parameters( model, participant, fit_results ) :\n",
    "    parameter_str = fit_results.loc[ ( fit_results[ 'model' ] == model.name ) & (fit_results[ 'participant' ] == participant ), 'parameter values' ].to_numpy()[0]\n",
    "    model.parameter = eval( parameter_str )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fe835-7d8e-4a8b-a700-e04af7fc2d7d",
   "metadata": {},
   "source": [
    "#### 5.2 Simulation implementation\n",
    "Each participant has a different behavior. So, for each model, we ran k simulations per participant using individual parameters. \n",
    "\n",
    "Considering 4 models (Random, CK, RW and RWCK), 5 participants and k = 3 simulations per participant / model, we have to run already 60 simulations. Note that k=3 is a small number, I used 50 simulations in my project.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> implement the function <i>simulate()</i>. You should use the methods <i>action_probability</i> and <i>update</i> from the class Model as well as the method <i>random.choice</i> from numpy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d420-a52c-4bcd-9824-76be3d1592d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions( threshold = sys.maxsize )\n",
    "actions = np.array([Action.MENU, Action.SHORTCUT, Action.LEARNING] )\n",
    "penalty = 3.0 #The study includes a 3s penalty when an error occurs\n",
    "\n",
    "#####################\n",
    "# For each participant and each model, run k simulations\n",
    "# INPUTS\n",
    "#   model_vec       : list of the models to simulate\n",
    "#   all_df          : empirical data containing in particular the sequence of commmands for each participant\n",
    "#   fitting_results : dataframe containing the set of best parameters for each model and individual\n",
    "#   repetition      : number of simulations per model and participants\n",
    "# OUTPUT\n",
    "#   simulation_df   : dataframe extending all_df with all simulations\n",
    "#####################\n",
    "def simulate_all( model_vec, all_df, fitting_results, repetition ) :\n",
    "    simulation_vec = []                                # contain results of the simulations\n",
    "    participant_vec = all_df[ 'participant' ].unique() #list of the participants\n",
    "    print( participant_vec )\n",
    "    \n",
    "    for i in range(0, repetition) :\n",
    "        print( \"simulation\", i+1 , \"/\", repetition)\n",
    "        \n",
    "        for model in model_vec :\n",
    "            \n",
    "            for p in participant_vec :\n",
    "                \n",
    "                model.reset()\n",
    "\n",
    "                # select the best set of parameters of the participant p\n",
    "                set_best_parameters( model, p, fitting_results )\n",
    "\n",
    "                # select the sequence of commands of the participant p\n",
    "                command_vec = all_df.loc[ all_df.participant == p, 'cmd_input' ].to_numpy()\n",
    "\n",
    "                # run one simulation on the sequence of commands of participant p\n",
    "                strategy_vec, time_vec, success_vec = simulate( model, command_vec )\n",
    "\n",
    "                # create a datframe with the same format than the one of empirical data, but with\n",
    "                # the generated data\n",
    "                res_df = all_df.copy() \n",
    "                res_df = res_df[ res_df.participant == p ]                \n",
    "                res_df = res_df.assign( simulation = i, model = model.name, strategy = strategy_vec, time = time_vec, success = success_vec )\n",
    "                simulation_vec.append( res_df ) \n",
    "    \n",
    "    return pd.concat( simulation_vec )\n",
    "\n",
    "#####################\n",
    "# For each command, return an action, time and success\n",
    "# INPUTS\n",
    "#   model   : list of the models to simulate\n",
    "#   C       : sequence of commands\n",
    "# OUTPUT\n",
    "#   [X,T,S] : array containing the sequences of (1) actions, (2) time and (3) success\n",
    "#####################\n",
    "def simulate( model, C ): \n",
    "     \n",
    "    X = np.array( [], dtype = int)   # sequence of action\n",
    "    T = np.array( [], dtype = float) # sequence of time\n",
    "    S = np.array( [], dtype = int )  # sequence of success \n",
    "\n",
    "    for command in C :\n",
    "\n",
    "        ####################\n",
    "        # TODO\n",
    "        # acion = ...\n",
    "        # t = ...\n",
    "        # success = ...\n",
    "        ####################\n",
    "\n",
    "        X = np.append( X, action)\n",
    "        T = np.append( T, t)\n",
    "        S = np.append( S, success)\n",
    "        \n",
    "    return [ X, T, S ]\n",
    "\n",
    "######################\n",
    "# run the simulation #\n",
    "######################\n",
    "simulation_df = simulate_all( model_vec, all_df, result_df, 7 )\n",
    "print( \"simulations...finished\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc225659-86de-499e-948b-084ef75da22a",
   "metadata": {},
   "source": [
    "#### 5.3 Simulation visualization\n",
    "You can now visualize the results of the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cd2d1-1729-4597-8350-eb8bb2a13cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "#import math\n",
    "\n",
    "#######################################\n",
    "def distance_str(y_observation, y_prediction ):\n",
    "    ############\n",
    "    # TODO\n",
    "    # return 'RMSE:' + str( ... )\n",
    "    return \"\"\n",
    "    ############\n",
    "\n",
    "######################################\n",
    "def display_simulations( all_df, _simulation_df ) :\n",
    "    simulation_df = _simulation_df.copy()\n",
    "    simulation_df[ 'correct_hotkey'] = 0\n",
    "    simulation_df.loc[ ( simulation_df[ 'success'] == 1) & (simulation_df[ 'strategy' ] == 1), 'correct_hotkey' ] = 100\n",
    "    model_vec     = simulation_df[ 'model' ].unique()\n",
    "    n_simulations = simulation_df[ 'simulation' ].max()\n",
    "\n",
    "    # agregate observed data per block and participant for visualisation\n",
    "    observation_df = all_df.copy()\n",
    "    observation_df[ 'correct_hotkey' ] = 0\n",
    "    observation_df.loc[ ( observation_df[ 'success'] == 1) & (observation_df[ 'strategy' ] == 1), 'correct_hotkey' ] = 100\n",
    "    observation_df = observation_df.groupby( [ 'block_id' , 'participant' ] ).mean( 'correct_hotkey' ).reset_index()\n",
    "    \n",
    "    #agregate observed data per block for estimating MSE\n",
    "    y_observation = observation_df.groupby( ['block_id' ] ).mean( 'correct_hotkey' ).reset_index()[ 'correct_hotkey' ].to_numpy()\n",
    "    #print( \"observation: \", y_observation)\n",
    "    \n",
    "    f, axes = plt.subplots(1, len( model_vec) )    \n",
    "    for i in range( 0, len( model_vec) ) :\n",
    "        model_df = simulation_df.copy()\n",
    "        model_df = model_df[ model_df.model == model_vec[i] ]\n",
    "        #print( model_df )\n",
    "        model_df = model_df.groupby( [ 'block_id' , 'participant' ] ).mean( 'correct_hotkey' ).reset_index()\n",
    "        #print( model_df )\n",
    "        y_prediction = model_df.groupby( [ 'block_id' ] ).mean( 'correct_hotkey' ).reset_index()[ 'correct_hotkey' ].to_numpy() \n",
    "        #print( \"prediction: \", y_prediction)\n",
    "        \n",
    "        rel  = sns.lineplot( data = model_df        , x = 'block_id', y = 'correct_hotkey', linewidth = 3, color = \"green\", ax = axes[i] )\n",
    "        rel2 = sns.lineplot( data = observation_df  , x = 'block_id', y = 'correct_hotkey', linewidth = 3, color = \"green\", ax = axes[i], linestyle='--' )\n",
    "        axes[i].text(1, 90, distance_str(y_observation, y_prediction) , fontsize=16)\n",
    "        info = rel.set( title= str( n_simulations ) + ' simulations of ' + model_vec[ i ], xlabel = \"Block id\", ylabel= \"Shortcut use (%)\", ylim = [0,100], xlim=[0,14] )\n",
    "\n",
    "display_simulations( all_df, simulation_df )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91657643-1772-49e4-b97e-c530dd63f633",
   "metadata": {},
   "source": [
    "The figure above shows the shortcut use (%) per block for each model. Observed participants’ data are represented with dots. Synthetised data (solid line) are produced by aggregating n simulations per participant with individual parameters (ie. ther results of your simulations).\n",
    "\n",
    "The results are interesting beacuse they show a different picture. While model fit results indicated that Random and CK model better accounts for the empirical data, the <strong>visual inspection</strong> of simulation results suggest that only RW better synthetizes users’ behavioral data (in term of increasing of shortcut use).\n",
    "\n",
    "To complete visual inspection, we can use RMSE to quantify how closely model simulations reflects participants behavior.\n",
    "\n",
    "#### 5.4 Root Mean Square Error (RMSE)\n",
    "Multiple metrics (MAE, MSE, RMSE, R2) are available to evaluate the performance of the model in regression analysis ( \n",
    "https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e)\n",
    "\n",
    "In this exercice, we will use the Root Mean Square Error (RMSE) because: \n",
    "- It penalizes the large prediction errors\n",
    "- It is widely used\n",
    "- It has the same units as the dependent variable (Y-axis).\n",
    "\n",
    "RMSE is the square root of value obtained from Mean Square Error (MSE) function. MSE is the average squared difference between the estimated values (prediction) and the actual value (observation):\n",
    "\n",
    "$$ RMSE = \\sqrt{MSE} = \\sqrt{ \\frac{1}{n} \\sum_1^n (y_i^{pred} - {y_i^{obs}})^2 }$$\n",
    "\n",
    "where \n",
    "- $n$ is the number of predictions / observations\n",
    "-  $\\{y_0^{obs}, ..., y_n^{obs} \\}$, the vector of observed values\n",
    "-  $\\{y_0^{pred}, ..., y_n^{pred} \\}$, the vector of predicted values\n",
    "\n",
    "The lower value of RMSE, the better it is.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> update the <i>distance_str()</i> function above to compute RMSE. You can do it manually or use the method <i>mean_square_error</i> from scipy. Visualize the data.\n",
    "</div>\n",
    "\n",
    "#### 5.5 Conclusion\n",
    "\n",
    "In conclusion, these results\n",
    "- highlight the importance of combining model fitting and model simulation to validate HCI models as they show a different picture.\n",
    "- highlight the iportance of studying model fit at the population level as well as the individual level \n",
    "- suggest that this first set of classical models are **not satisfactory** as they tend to overestimate initial shortcut use and underestimate the final shortcut use. This motivates us to elaborate a dedicated model of shortcut adoption [Bailly et al. 2023].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a80b3b-1645-4185-a6e4-6decac3e7ad8",
   "metadata": {},
   "source": [
    "# Part 6: Step-by-step model evaluation (30mn)\n",
    "In this part, we explain the step-by-step process to design and evaluate HCI models. It is again based on the article of Wilson and Collins. The process is illustrated in the figure below (extracted from their article).\n",
    "\n",
    "<center><img src=\"images/pipeline.pdf\" width=\"700\"/></center> \n",
    "\n",
    "<center><strong>Figure 7:</strong> Adapted fron Wilson and Collins. Process for using computional modeling to better understand behavior. </center> \n",
    "\n",
    "<p style=\"background-color:lightgreen;\">\n",
    "In the previous parts, we mainly focused on the green box:</p>\n",
    "<ul > \n",
    "    <li>fit real data</li>\n",
    "<li>parameter fits</li>\n",
    "<li>model comparison</li>\n",
    "<li>validate the model (simulate the model with the best parameters)</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "The objective is:\n",
    "- to quickly discuss the other cells\n",
    "- and to focus on parameter recovery and model recovery\n",
    "\n",
    "<p style=\"background-color:lightpink;\">-</p>\n",
    "\n",
    "#### 6.1 Design experiment\n",
    "\n",
    "Computational modeling does not replace good experimental design. When building a model, you should design the experiment <strong>at the same time</strong>. You should ask yourself the following questions to optimize your experimental design (and your model):\n",
    "<ul > \n",
    "<li>What scientific question are you asking?</li>\n",
    "<li>Does your experiment engage the targeted processes?</li>\n",
    "<li>Will signatures of the targeted processes be evident from simple statitics of the data?</li>\n",
    "</ul>\n",
    "\n",
    "One key message here is that the experiment should be designed before or at the same time as the model, not after!\n",
    "\n",
    "\n",
    "<p style=\"background-color:lightpink;\">-</p>\n",
    "\n",
    "#### 6.2 Build models\n",
    "\n",
    "Recommendations are :\n",
    "<ul> \n",
    "<li>A computation model should be as simple as possible, bu no simpler</li>\n",
    "<li>A computaional model should be interpretable</li>\n",
    "<li>The models should capture all the hypotheses you plan to test</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p style=\"background-color:#F5CBA7;\">- </p>\n",
    "\n",
    "#### 6.3 Simulate, simulate, simulate\n",
    "\n",
    "Previously we simulated the model(s) <strong>after</strong> having collected empirical data and fitted the parameters. It is also very important to simulate the models <strong>before</strong> collecting empirical data, ie. to create fake, or surrogate data. Indeed, you should use the models to simulate the behavior of participants in the experiment, and to observe how behavior changes with different models, different model parameters, and different variants of the experiment. \n",
    "\n",
    "This step will allow you to refine the first two steps (6.1 and 6.2): confirming that the experimental design elicits the behaviors assumed to be captured by the computational model. To do this, here are some important steps.\n",
    "\n",
    "<ul > \n",
    "<li>Define model-independent measures that capture key aspects of the processes you are trying to model. In our case, it would have been the evolution of shortcut use</li>\n",
    "<li>Simulate the model across the range of parameter values</li>\n",
    "<li>Visualize the simulated behavior of different models.</li>\n",
    "</ul>\n",
    "\n",
    "For instance, imagine that we did **not** collect empirical data. We could still have some assumptions regarding the evolution of shortcut use, ie. shortcut use increases with practice. We can then simulate the models with different \"plausible\" sets of parameters and check whether the generated behaviors are plausible.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO: </strong> To save time, we only simulate the Random model with different values of the parameter $\\theta$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e8ef6-cb57-43e5-b31c-569295303fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# model              : model to simulate\n",
    "# parameter_set_list : list containing different parameter sets for model \n",
    "# C                  : sequence of commands\n",
    "# block_id           : block ids (used to aggregate data)\n",
    "#####################\n",
    "def simulation_preview( model, parameter_set_list, C, block_id, repetition ) :\n",
    "    simulation_vec = []\n",
    "    \n",
    "    for i in range( 0, repetition ) :\n",
    "        print( \"simulation\", i+1 , \"/\", repetition)\n",
    "            \n",
    "        for parameter_set in parameter_set_list :\n",
    "            model.reset()\n",
    "            model.parameter = parameter_set\n",
    "\n",
    "            # simulate the model\n",
    "            X, T, S = simulate( model, C )\n",
    "\n",
    "            # save simulation results\n",
    "            data = {'model'     : model.name,\n",
    "                    'parameters': str( parameter_set ) ,\n",
    "                    'cmd_input' : C, 'block_id' : block_id,\n",
    "                    'strategy'  : X, 'time' : T, 'success': S } \n",
    "            simulation_vec.append( pd.DataFrame( data ) ) \n",
    "    \n",
    "    return pd.concat( simulation_vec )\n",
    "\n",
    "\n",
    "#####################\n",
    "# return list of parameter sets\n",
    "#####################\n",
    "def parameter_sets( parameter_names, parameter_values, res_vec, index = 0, current = dict() ):\n",
    "    if index == len( parameter_names ) :\n",
    "        res_vec.append( current.copy() )\n",
    "        return res_vec\n",
    "\n",
    "    name   = parameter_names [ index ] \n",
    "    print( name )\n",
    "    values = parameter_values[ index ]\n",
    "    for value in values :\n",
    "        current[ name ] = value\n",
    "        res_vec = parameter_sets( parameter_names, parameter_values, res_vec, index + 1, current )\n",
    "   \n",
    "    return res_vec\n",
    "\n",
    "####################\n",
    "####################\n",
    "\n",
    "# get the sequence of commands (and blocks) of the participant 5\n",
    "C            = all_df.loc[ all_df.participant == 5, 'cmd_input' ].to_numpy()\n",
    "block_id     = all_df.loc[ all_df.participant == 5, 'block_id'  ].to_numpy()\n",
    "Random_model = model_vec[ 0 ]\n",
    "\n",
    "# create the list of parameters to test\n",
    "parameter_set_list = []\n",
    "parameter_names = [ 'THETA' ]\n",
    "parameter_values = [ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] ]\n",
    "parameter_set_list = parameter_sets( parameter_names, parameter_values, parameter_set_list )\n",
    "print( parameter_set_list )\n",
    "\n",
    "# simulate the random model with four different values of theta: parameter_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# TODO\n",
    "#rand_model_simulation = simulation_preview( Random_model, parameter_set_list, \n",
    "#                                           C = C, block_id = block_id, \n",
    "#                                           repetition = 5 )\n",
    "print( \"simulation previews finished\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5fd50-0f69-4856-91ca-d1f43311eaab",
   "metadata": {},
   "source": [
    "We can now visualize the results of our simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fa49b-0f75-49e1-9551-7e67b21bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "def display_simulations( _simulation_df ) :\n",
    "    simulation_df = _simulation_df.copy()\n",
    "    simulation_df[ 'correct_hotkey'] = 0\n",
    "    simulation_df.loc[ ( simulation_df.success == 1 ) & ( simulation_df.strategy == 1 ), 'correct_hotkey' ] = 100\n",
    "    simulation_df = simulation_df.groupby( [ 'block_id' , 'parameters' ] ).mean( 'correct_hotkey' ).reset_index()\n",
    "    rel  = sns.lineplot( data = simulation_df, x = 'block_id', y = 'correct_hotkey', hue='parameters', linewidth = 3 )\n",
    "    info = rel.set( title= ' simulations of Random with different values of theta', xlabel = \"Block id\", ylabel= \"Shortcut use (%)\", ylim = [0,100], xlim=[0,14] )\n",
    "\n",
    "display_simulations( rand_model_simulation )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c09b48-3f84-4f2c-bbb5-d28fae38f04f",
   "metadata": {},
   "source": [
    "It appears clearly at this stage **this model is not sufficient to explain and predict shortcut adoption**. So, no need to collect data and fit parameters for this model. Instead you should consider another model-independent measures, experiment design and more likely another model...\n",
    "\n",
    "<p style=\"background-color:#F5CBA7;\">- </p>\n",
    "\n",
    "#### 6.4 Parameter recovery\n",
    "This step is often neglected in HCI. Before reading too much into the best fitting parameter values, it is important to check whether the fitting procedure gives meaningful parameter values in the best case scenario otherwise, it is likely that you have a problem somewhere. The process is explained below. It consists of simulating the model with ''arbitrary parameters'' and to check wether the parameter fit procedure on the simulated data estimate the same parameter values.\n",
    "\n",
    "<center><img src=\"images/parameter_recovery.pdf\" width=\"900\"/></center> \n",
    "\n",
    "<center><strong>Figure 8:</strong> Parameter recovery </center> \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>TODO:</strong> test whether you can recover parameters for the RW model </strong>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad5333-1552-4929-9732-02105f74ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# The function returns a random valid set of parameters for the model 'model'\n",
    "def random_parameter_set( model ) :\n",
    "    res = dict()\n",
    "    names  = model.parameter_names()\n",
    "    bounds = model.parameter_bounds()\n",
    "    for i in range( 0, len( names ) ) :\n",
    "        res[ names[i] ] = round( random.uniform( bounds[i][0], bounds[i][1] ), 2 )\n",
    "    return res\n",
    "\n",
    "                             \n",
    "#####################\n",
    "# model      : model\n",
    "# parameter_set_list : list containing the set of parameter of model\n",
    "# C          : sequence of commands (command_vec)\n",
    "# block_id   : \n",
    "# repetition : nb of simulation per parameter set\n",
    "#####################\n",
    "def parameter_recovery( model, C, block_id, repetition ) :\n",
    "    simulation_vec = []\n",
    "    \n",
    "    for i in range(0, repetition) :\n",
    "            \n",
    "        model.reset()\n",
    "\n",
    "        ###############\n",
    "        # TODO\n",
    "        # model.parameter = ...\n",
    "        # ... = simulate( ... )\n",
    "        ###############\n",
    "        \n",
    "\n",
    "        # parameter fit\n",
    "        param_names = model.parameter_names()\n",
    "        bounds      = model.parameter_bounds()\n",
    "\n",
    "        ###############\n",
    "        # TODO\n",
    "        # res = differential_evolution( ... )\n",
    "        ###############\n",
    "        \n",
    "        \n",
    "        fit_parameter_set = dict( zip(param_names, np.round( res.x, 1 ) ) )\n",
    "        print( \"sim\", i+1 , \"/\", repetition, '\\t', model.parameter, fit_parameter_set)\n",
    "            \n",
    "        #output\n",
    "        data = {'model'     : model.name, \n",
    "                'simulated parameters': str( model.parameter ),\n",
    "                'fit parameters': str( fit_parameter_set ),\n",
    "                'simulation': i }\n",
    "        for name in model.parameter.keys() :\n",
    "            data[ name ] = model.parameter[ name ]\n",
    "            data[ name + '_r' ] = fit_parameter_set[ name ]\n",
    "\n",
    "        simulation_vec.append( pd.DataFrame( data, index=[0] ) )\n",
    "\n",
    "    return pd.concat( simulation_vec )\n",
    "\n",
    "\n",
    "################\n",
    "################\n",
    "model = model_vec[ 2 ] # RW model\n",
    "parameter_recovery_df = parameter_recovery( model, C, block_id, 40)\n",
    "parameter_recovery_df.to_csv( './data/parameter_recovery.csv' )\n",
    "print( \"finished\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce3678-33b0-47cf-b424-3de89b7a32a8",
   "metadata": {},
   "source": [
    "We can now visualize the results of the parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0831db4-36b8-40f9-b056-934dd3a4cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# recovery_df  : dataframe with (at least) four columns\n",
    "# parameter_names  : array containing the name of the parameters\n",
    "# Note, for each parameter p, there is a column 'p' and 'p_r'\n",
    "#############################################\n",
    "def plot_parameter_recovery_distribution( recovery_df, parameter_names ) :\n",
    "    f, axes = plt.subplots( 1, len( parameter_names) )\n",
    "    df = recovery_df.copy()\n",
    "    df = df.reset_index()\n",
    "    for i, name in enumerate( parameter_names ) : \n",
    "         df [ name + '_diff' ] = df[ name + '_r' ] - df[ name ]\n",
    "         #print( df )\n",
    "         rel  = sns.histplot( data = df, x = name + '_diff', binwidth=0.05, stat='percent',  ax= axes[ i ]  )\n",
    "         info = rel.set( xlabel = 'fit ' + name + ' - simulated ' + name )\n",
    "\n",
    "######################################\n",
    "# recovery_df  : dataframe with (at least) four columns\n",
    "# parameter_names  : array containing the name of the parameters\n",
    "# Note, for each parameter p, there is a column 'p' and 'p_r'\n",
    "#############################################\n",
    "def plot_parameter_recovery_fit( recovery_df, parameter_names ) :\n",
    "    f, axes = plt.subplots( 1, len( parameter_names) )\n",
    "    df = recovery_df.copy()\n",
    "    df = df.reset_index()\n",
    "    for i, name in enumerate( parameter_names ) : \n",
    "         df [ name + '_diff' ] = df[ name + '_r' ] - df[ name ]\n",
    "         #print( df )\n",
    "         rel  = sns.regplot( data = df, x = name, y = name + '_r', ax= axes[ i ]  )\n",
    "         info = rel.set( xlabel = 'simulated ' + name, ylabel = 'fit ' + name )\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# recovery_df  : dataframe with (at least) four columns\n",
    "# parameter_names  : array containing the name of the parameters len( param_names ) ==2\n",
    "# This function only works for models with two parameters.\n",
    "#############################################\n",
    "def plot_parameter_recovery_fit_bis(recovery_df, parameter_names) :\n",
    "    bins = 4\n",
    "    print( recovery_df[ parameter_names[0] ].unique() )\n",
    "    print( recovery_df[ parameter_names[1] ].unique() ) \n",
    "    #unique_values = np.array( [ recovery_df[ parameter_names[0] ].unique(), recovery_df[ parameter_names[1] ].unique() ] )\n",
    "    unique_values = [ recovery_df[ parameter_names[0] ].unique(), recovery_df[ parameter_names[1] ].unique() ]\n",
    "    \n",
    "    min_values = np.array( [np.min( unique_values[0] ), np.min( unique_values[1] ) ] )\n",
    "    max_values = np.array( [np.max( unique_values[0] ), np.max( unique_values[1] ) ] )\n",
    "    span = (max_values - min_values ) / bins\n",
    "    \n",
    "    f, axes = plt.subplots( 2, bins )\n",
    "\n",
    "    for index in [0,1] :\n",
    "        name1 = parameter_names[ index ]\n",
    "        name2 = parameter_names[ 1 - index ]\n",
    "\n",
    "        for i in range(0, bins):\n",
    "            df = recovery_df.copy()\n",
    "            delta = round( min_values[ index ] + i * span[ index ], 1) \n",
    "            df = df[ ( df[ name1 ] >= delta ) & ( df[ name1] <= round( delta + span[ index ],1) ) ]\n",
    "            min_ = np.min( np.concatenate( (df[ name2 ].to_numpy() , df[ name2 + '_r' ].to_numpy()) ) ) -0.05\n",
    "            max_ = np.max( np.concatenate( (df[ name2 ].to_numpy() , df[ name2 + '_r' ].to_numpy()) ) ) + 0.05\n",
    "            rel  = sns.regplot( data = df, x = name2 , y = name2 + '_r', ax = axes[index,i] )\n",
    "            info = rel.set( title= name1 + '(' + str( delta )+',' + str( delta+ span[index] )  + ')' , xlabel = 'simulated ' + name2, ylabel = 'fit ' + name2, xlim = [min_, max_], ylim = [min_, max_] )\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "plt.rcParams[\"figure.figsize\"] = [8.0, 4.0]\n",
    "plot_parameter_recovery_distribution( parameter_recovery_df, ['A_RW', 'B_RW'] )\n",
    "plot_parameter_recovery_fit( parameter_recovery_df, ['A_RW', 'B_RW'] )\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16.0, 4.0]\n",
    "plot_parameter_recovery_fit_bis( parameter_recovery_df, ['A_RW', 'B_RW'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d672dd-3b41-4e0c-8d43-0997853be719",
   "metadata": {},
   "source": [
    "Ideally, the recovered parameter value is the same one than the parameter value used for the simulation. So their difference should be 0. \n",
    "\n",
    "Given the RW model, if A_RW is close to 0 or 1, it will be difficult to recover B_RW. Similarly if B_RW is close to 0 or larger than 1, it will be difficult to recover A_RW. \n",
    "\n",
    "\n",
    "#### 6.5 Model recovery\n",
    "Previously we see that parameter fitting should be validated by parameter recovery on simulated data. Similarly, model comparison should be validated by model recovery on simulated data. The process is the following one:\n",
    "\n",
    "<center><img src=\"images/model_recovery.pdf\" width=\"900\"/></center> \n",
    "\n",
    "<center><strong>Figure 9:</strong> Model recovery </center> \n",
    "\n",
    "- simulate data from all models with a range of paramter values\n",
    "- fit that data with all models\n",
    "- determine the extent to which fake data generated from model A is best fit by model A as opposed to model B.\n",
    "- plot the confusion matrix showing the probability that each model is the best fit to data generated from the other models.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong> TODO: </strong> implement model revovery\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d52435-7d11-484f-b6b0-25684c2cbfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "######################\n",
    "# model_vec      : set of models to compare\n",
    "# C              : command vec\n",
    "# nb_simulations : nb of simulations per model\n",
    "######################\n",
    "def model_recovery( model_vec, C, nb_simulations ) :\n",
    "    res_vec = []\n",
    "    for simulated_model in model_vec :\n",
    "        \n",
    "        for i in range( 0, nb_simulations ) :\n",
    "            simulated_model.reset()\n",
    "\n",
    "            simulated_model.parameter = random_parameter_set( simulated_model )\n",
    "\n",
    "            # run simulation (X sequences of strategy, T: sequence of time, S: sequence of success)\n",
    "            X, T, S = simulate( simulated_model, C )\n",
    "            \n",
    "            for fit_model in model_vec :\n",
    "                # fitting\n",
    "                fit_model.reset()\n",
    "                param_names = fit_model.parameter_names()\n",
    "                bounds      = fit_model.parameter_bounds()\n",
    "\n",
    "                ##########\n",
    "                # TODO\n",
    "                # res = differential_evolution( ... )\n",
    "                ##########\n",
    "                                \n",
    "                \n",
    "                ll  = - round( res.fun, 1)\n",
    "                BIC =  round( bic_score( len(X) , len( param_names) , ll), 1 ) \n",
    "                \n",
    "                #output\n",
    "                data = {'simulated model' : simulated_model.name,\n",
    "                    'fit model'       : fit_model.name,\n",
    "                    'BIC'             : BIC,\n",
    "                    'simulation': i }\n",
    "                print( data )\n",
    "\n",
    "                res_vec.append( pd.DataFrame( data, index=[0] ) )\n",
    "    return pd.concat( res_vec )\n",
    "\n",
    "#######################\n",
    "#######################\n",
    "#######################\n",
    "random_model = model_vec[ 0 ]\n",
    "RW_model     = model_vec[ 2 ]\n",
    "model_recovery_table = model_recovery_table = model_recovery( model_vec, C, 5 )\n",
    "print( \"finished\" )\n",
    "model_recovery_table.to_csv( './data/model_recovery.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460cf78-5210-4e10-8571-11aa70811fed",
   "metadata": {},
   "source": [
    "Build the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1aa77-cb69-486d-af17-a67af15ebbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_recovery_table = pd.read_csv( './data/model_recovery.csv' )\n",
    "n_simulations = model_recovery_table[ 'simulation' ].max() +1\n",
    "\n",
    "# get confusion matrix\n",
    "idx = model_recovery_table.groupby( ['simulated model', 'simulation'] )[ 'BIC' ].idxmin()\n",
    "model_recovery_table = model_recovery_table.loc[ idx ]\n",
    "confusion_matrix_long = model_recovery_table.groupby( ['simulated model', 'fit model' ] )['BIC'].count().reset_index(name=\"count\")\n",
    "confusion_matrix_long['percent'] = 100*confusion_matrix_long['count'] / n_simulations \n",
    "confusion_matrix = confusion_matrix_long.pivot(index='simulated model', columns='fit model', values='percent').fillna(0)\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.rcParams[\"figure.figsize\"] = [5.0, 4.0]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.heatmap( confusion_matrix, fmt='1g', annot= True, cmap='Blues')\n",
    "plt.yticks(rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7d13f-e319-4dc4-9025-38883805d4e3",
   "metadata": {},
   "source": [
    "We observe that RW and RWCK are hard to distinguish. So comparing them does not make so much sense in our context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b52152-0538-4495-ac54-ea0bc4e38959",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"images/pipeline.pdf\" width=\"700\"/></center> \n",
    "\n",
    "<center><strong>Figure 7:</strong> Adapted fron Wilson and Collins. Process for using computional modeling to better understand behavior. </center> \n",
    "\n",
    "\n",
    "# END\n",
    "You are now ready to properly evaluate and compare computational models in HCI!\n",
    "Thanks for your participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b5b5d-f3af-4a55-9e57-1709023b5ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
