{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of how to use Embeddings.\n",
    "\n",
    "Embeddings are a way to represent information (such as words, paragraphs, images, etc.) in a vector space. This type of numerical reprsentation is useful because it let us use mathematical operations in _semantic space_. For example, we can find the similarity between two paragraphs words by calculating the cosine similarity between their embedding vectors.\n",
    "\n",
    "This notebook will illustrate how to use embeddings from a local model - using Ollama - and from an external model - using OpenAI and HuggingFace.\n",
    "\n",
    "## Using a local model\n",
    "\n",
    "Using a local model has the advantage of being able to run the model offline. The disadvantage is that the model may not be as good as a model from a large company like OpenAI.\n",
    "\n",
    "To install Ollama, follow the instructions at their [download homepage](https://ollama.com/download).\n",
    "\n",
    "Once you have Ollama installed, you need to dpwnload a model. You can see the available models from the [Ollama models page](https://ollama.com/models). For example, to install _llama3_ you will type on your terminal.\n",
    "\n",
    "```bash\n",
    "ollama pull mistral\n",
    "```\n",
    "\n",
    "You can then use the model directly using ollama's [RESTful API](https://github.com/ollama/ollama/blob/main/docs/api.md).\n",
    "\n",
    "In this notebook, we will show how to use [LangChain](https://python.langchain.com/v0.2/docs/introduction/) to interact with the model embedings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read secrets ------------------------------------------------\n",
    "import json\n",
    "import os\n",
    "\n",
    "# you define your on secrets.json file with the following structure\n",
    "# {\n",
    "#     \"openai\": \"your-openai-api-key\",\n",
    "#     \"groq\": \"your-groq-api-key\"\n",
    "# }\n",
    "\n",
    "with open(\"./secrets.json\") as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"openai\"]\n",
    "os.environ[\"GROQ_API_KEY\"] = secrets[\"groq\"]\n",
    "\n",
    "# this is tp disable some warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "# Lets show what an embeddding vector looks like (first 5 elements)\n",
    "query_result[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using an embedding function to find the document (part) most likely to have the information we need to answer a question.\n",
    "\n",
    "In this example we will first load a number of documents from a directory. We will them split them into chunks - we do this because:\n",
    "\n",
    "- The context window of a model may be limited, so we may need to split the document into smaller parts.\n",
    "- Documents are not monolitic semantically, so we may need to split them into parts that are more coherent.\n",
    "\n",
    "We then use these chunks to populate a vector database. We will use the vector database to find the most similar chunk to a phrase or question. The quality of the result (how actually similar a chunk is to a phrase) depens on the quality of the embedding function (Transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents from a directory\n",
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import  DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('./documents', glob=\"**/*.html\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "# Splitting documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the chunks into a vector database\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the store to retrieve the (parts of a) document that is likely to contain the answer to a question\n",
    "\n",
    "question = \"what is the personal fitness market size?\"\n",
    "results = db.similarity_search(question, k=3)\n",
    "\n",
    "# I want to make results pretty\n",
    "# use map and lamda to transform the results into a list of dictionaries\n",
    "results = list(map(lambda x: {\"text\": x.page_content, 'metadata': x.metadata}, results))\n",
    "\n",
    "print(json.dumps(results, indent=2, sort_keys=True))\n",
    "\n",
    "# hopefully the top documemnt contains the information needed to answer the question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# model = Ollama(model=\"llama3\", temperature=0.25)\n",
    "model = ChatGroq(temperature=0.25, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Answer the question: {question} using the information from this context: {context}\"\n",
    ")\n",
    "\n",
    "# put together a retrieval chain\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer-school-24-wpJ3waSW-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
